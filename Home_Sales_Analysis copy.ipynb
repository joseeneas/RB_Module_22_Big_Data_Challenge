{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 22 - BIG DATA ANALYSIS WITH SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 00 - Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "notebookRunGroups": {
     "groupValue": ""
    },
    "outputId": "78c91763-b7de-43b3-ae23-1fb13e566086",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Step 00 - Environment Setup\n",
    "#\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil          as shu\n",
    "import datetime\n",
    "import warnings   \n",
    "import findspark    \n",
    "from   colorama    import Fore                       \n",
    "from   pyspark     import SparkFiles  \n",
    "from   pyspark.sql import SparkSession\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "w, h       = shu.get_terminal_size()\n",
    "\n",
    "def logStep(msg):\n",
    "    l1 = len(msg)\n",
    "    l2 = w - l1\n",
    "    print(Fore.WHITE + str(datetime.datetime.now()) +  Fore.YELLOW + ' STEP ' + msg + Fore.WHITE + '-' * l2  )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "logStep('00 - ENVIRONMENT PREPARATION')\n",
    "warnings.filterwarnings('ignore')\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def spark_SQL_Run(step_Number, step_Message1, sql_Statement):\n",
    "  start_time         = datetime.datetime.now()\n",
    "  logStep(step_Number + ' - ' + step_Message1)\n",
    "  print()\n",
    "  try:\n",
    "    spark.sql(sql_Statement).show(5)\n",
    "  except Exception as e:\n",
    "    logStep(F\"{step_Number} - Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "  else:\n",
    "    logStep(step_Number + ' - ' + 'Spark SQL Statement Executed Successfully')\n",
    "  logStep(step_Number + ' - ' + 'DONE')\n",
    "  end_time           = datetime.datetime.now()\n",
    "  step_elapsed_time  = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "def runtime_Diff(step_Number, step_Message1,base_SQL, cached_SQL):\n",
    "  start_time          = datetime.datetime.now()\n",
    "  logStep(step_Number + \" - RUNTIME DIFFERENCE\")\n",
    "  time_difference     = base_SQL - cached_SQL\n",
    "  logStep(F\"{step_Number} - Time required for a non-cached Query : {base_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time required for a cached/part Query: {cached_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time difference                      : {time_difference}\")\n",
    "  logStep(step_Number + \" - DONE\")\n",
    "  end_time            = datetime.datetime.now()\n",
    "  step_elapsed_time   = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "print(F'Copyright        : {sys.copyright}')\n",
    "print(F'OS Platform      : {sys.platform}')\n",
    "print(F'OS Name          : {os.name}')\n",
    "print(F'OS HOME          : {os.environ.get(\"HOME\")}')\n",
    "print(F'OS uName         : {os.uname().sysname}')\n",
    "print(F'OS NodeName      : {os.uname().nodename}')\n",
    "print(F'OS Release       : {os.uname().release}')\n",
    "print(F'OS Release Ver   : {os.uname().version}')\n",
    "print(F'OS Machine       : {os.uname().machine}')\n",
    "print(F'Process ID       : {os.getpid()}')\n",
    "print(F'Parent Process   : {os.getppid()}')\n",
    "print(F'OS User          : {os.getlogin()}')\n",
    "print(F'OS User ID       : {os.getuid()}')\n",
    "print(F'OS Group ID      : {os.getgid()}')\n",
    "print(F'OS Effective ID  : {os.geteuid()}')\n",
    "print(F'OS Effective GID : {os.getegid()}')\n",
    "print(F'Current dir      : {os.getcwd()}')\n",
    "print(F'Python version   : {sys.version}')\n",
    "print(F'Version info     : {sys.version_info}')\n",
    "print(F'Python API Ver   : {sys.api_version}')\n",
    "print(F'Executable       : {sys.executable}')\n",
    "print(F'Hadoop home      : {os.environ.get(\"HADOOP_HOME\")}')\n",
    "print(F'Spark version    : {findspark.__version__}')\n",
    "print(F'Spark home(Find) : {findspark.find()}')\n",
    "print(F'Spark Home(Env)  : {os.environ.get(\"SPARK_HOME\")}')\n",
    "print(F'Spark UI         : http://localhost:4040')\n",
    "print(F'Spark submit     : {sys.argv[0]}')\n",
    "print(F'Java home        : {os.environ.get(\"JAVA_HOME\")}')\n",
    "\n",
    "logStep(\"00 - DONE\");\n",
    "end_time            = datetime.datetime.now()\n",
    "step00_elapsed_time = end_time - start_time\n",
    "logStep(F\"00 - ELAPSED TIME: {step00_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01 - Read in the source file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "69591c83-18c1-4e2a-86d4-2e5219c6d006",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Step 01 - Read in the source file into a DataFrame.\n",
    "#\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logStep(\"01 - READ SOURCE DATA\");\n",
    "\n",
    "try:\n",
    "\n",
    "  url   = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "  spark.sparkContext.addFile(url)\n",
    "  home_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True)\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"01 - Spark operation - Source data read successfully\")\n",
    "\n",
    "logStep(\"01 - DONE\");\n",
    "end_time           = datetime.datetime.now()\n",
    "step01_elapsed_time = end_time - start_time\n",
    "logStep(F\"01 - ELAPSED TIME: {step01_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02 - Create a temporary view of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoljcJ7WPpnm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Step 02 - Create a temporary view of the DataFrame.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"02 - CREATE A TEMPORARY VIEW\")\n",
    "\n",
    "try:\n",
    "  home_df.createOrReplaceTempView('home_sales')\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"02 - Spark operation - Temporary view created successfully\")\n",
    "\n",
    "logStep(\"02 - DONE\")\n",
    "end_time           = datetime.datetime.now()\n",
    "step02_elapsed_time = end_time - start_time\n",
    "logStep(F\"02 - ELAPSED TIME: {step02_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "7d9b6b22-c722-42dc-80c0-81e8810f7521",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step03_elapsed_time = spark_SQL_Run(\"03\", \"WHAT IS THE AVERAGE PRICE?=4BR\", \"SELECT YEAR(date) AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 4 GROUP BY YEAR(date) ORDER BY YEAR(date) DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "e1c36565-5164-4a9f-f36a-b2ad94522f5e"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step04_elapsed_time = spark_SQL_Run(\"04\", \"WHAT IS THE AVERAGE PRICE?=3BR/3B\", \"SELECT date_built AS YEAR , ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "9b51b8bc-8d1e-4a4e-ff50-f94a621891be"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step05_elapsed_time = spark_SQL_Run(\"05\", \"WHAT IS THE AVERAGE PRICE?=3R/3/2B\", \"SELECT date_built AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 AND floors == 2 AND sqft_living >= 2000 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "73e46678-fca9-434f-feb8-b2460e96e41c"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Ste 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "#\n",
    "\n",
    "step06_elapsed_time = spark_SQL_Run(\"06\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 07 - Cache the the temporary table home_sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "7a421ad4-699e-4ae1-c13e-3ac1379a3cd6"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 07 - Cache the the temporary table home_sales.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"07 - CACHE HOME DATA\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"cache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1) \n",
    "else:\n",
    "  logStep(\"07 - Spark operation - Cache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"07 - DONE\")\n",
    "step07_elapsed_time = end_time - start_time\n",
    "logStep(F\"07 - ELAPSED TIME: {step07_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 08 - Check if the table is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "3003ef79-0678-4a80-e79a-eac9fdfc8a6f"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 08 - Check if the table is cached.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"08 - IS THE DATA CACHED?\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "    logStep(\"08 - home_sales is not cached\")\n",
    "  else:\n",
    "    logStep(\"08 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "    logStep(F\"Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logStep(\"08 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"08 - DONE\")\n",
    "step08_elapsed_time = end_time - start_time\n",
    "logStep(F\"08 - ELAPSED TIME: {step08_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "a7aec34a-063b-46bc-b428-8660a32d9a58"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "#\n",
    "\n",
    "step09_elapsed_time = spark_SQL_Run(\"09\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAYsuFaZuCmw",
    "outputId": "c5787884-c516-4f61-9956-6edfc0dcbf9a"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 10 Determine the runtime and compare to the original runtime\n",
    "#\n",
    "\n",
    "step10_elapsed_time = runtime_Diff(\"10\", \"RUNTIME DIFFERENCE\", step06_elapsed_time, step09_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"11 - FORMATTED PARQUET\")\n",
    "\n",
    "try:\n",
    "  home_df.write.parquet('home_parquet', mode='overwrite',partitionBy='date_built')\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"11 - Spark operation - Parquet file created successfully.\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"11 - DONE\")\n",
    "step11_elapsed_time = end_time - start_time\n",
    "logStep(F\"11 - ELAPSED TIME: {step11_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12 - Read the parquet formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 12 - Read the parquet formatted data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"12 - READ PARQUET\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df = spark.read.parquet('home_parquet')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"12 - Spark operation - Parquet file read successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"12 - DONE\")\n",
    "step12_elapsed_time = end_time - start_time\n",
    "logStep(F\"12 - ELAPSED TIME: {step12_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13 - Create a temporary table for the parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 13 - Create a temporary table for the parquet data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"13 - CREATE PARQUET VIEW\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df.createOrReplaceTempView('parquet_temp_home')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  print(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"13 - Spark operation - Parquet view created successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"13 - DONE\")\n",
    "step13_elapsed_time = end_time - start_time\n",
    "logStep(F\"13 - ELAPSED TIME: {step13_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "5420b7c8-7f4b-404f-fc91-f70a06039866"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places. \n",
    "#\n",
    "\n",
    "step14_elapsed_time = spark_SQL_Run(\"14\", \"14 - REPEAT QUERY\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM parquet_temp_home GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 15 Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEkIu5yYuWlW",
    "outputId": "db861ec0-49c1-4855-86b4-f360d566302e"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Step 15 - Determine the runtime and compare it to the cached version.\n",
    "#\n",
    "\n",
    "step15_elapsed_time = runtime_Diff(\"15\", \"RUNTIME DIFFERENCE\", step06_elapsed_time, step14_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 16 - Un-cache the home_sales temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "5b8e7eaf-78eb-4eef-d29d-1be425668fd1"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 16 - Un-cache the home_sales temporary table.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"16 - UNCACHE\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"uncache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"16 - Spark operation - Uncache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"16 - DONE\")\n",
    "step16_elapsed_time = end_time - start_time\n",
    "logStep(F\"16 - ELAPSED TIME: {step16_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 17 - Check if the home_sales is no longer cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "a3d33ffb-4052-41c6-8040-bff5a22a2188"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Step 17 - Check if the home_sales is no longer cached\n",
    "#\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"17 - CACHE CHECK\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "      logStep(\"17 - home_sales is not cached\")\n",
    "  else:\n",
    "      logStep(\"17 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"17 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"17 - DONE\")\n",
    "step17_elapsed_time = end_time - start_time\n",
    "logStep(F\"17 - ELAPSED TIME: {step17_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logStep(\"TOTAL RUN TIME\")\n",
    "logStep(F\" 0:   {step00_elapsed_time} seconds\")\n",
    "logStep(F\" 1:   {step01_elapsed_time} seconds\")\n",
    "logStep(F\" 2:   {step02_elapsed_time} seconds\")\n",
    "logStep(F\" 3:   {step03_elapsed_time} seconds\")\n",
    "logStep(F\" 4:   {step04_elapsed_time} seconds\")\n",
    "logStep(F\" 5:   {step05_elapsed_time} seconds\")\n",
    "logStep(F\" 6:   {step06_elapsed_time} seconds - non-cached query\")\n",
    "logStep(F\" 7:   {step07_elapsed_time} seconds\")\n",
    "logStep(F\" 8:   {step08_elapsed_time} seconds\")\n",
    "logStep(F\" 9:   {step09_elapsed_time} seconds - cached query\")\n",
    "logStep(F\"10:   {step10_elapsed_time} seconds\")\n",
    "logStep(F\"11:   {step11_elapsed_time} seconds\")\n",
    "logStep(F\"12:   {step12_elapsed_time} seconds\")\n",
    "logStep(F\"13:   {step13_elapsed_time} seconds\")\n",
    "logStep(F\"14:   {step14_elapsed_time} seconds - parquet cached query\")\n",
    "logStep(F\"15:   {step15_elapsed_time} seconds\")\n",
    "logStep(F\"16:   {step16_elapsed_time} seconds\")\n",
    "logStep(F\"17:   {step17_elapsed_time} seconds\")\n",
    "logStep(F\"TT:   {step00_elapsed_time + step01_elapsed_time + step02_elapsed_time + step03_elapsed_time + step04_elapsed_time + step05_elapsed_time + step06_elapsed_time + step07_elapsed_time + step08_elapsed_time + step09_elapsed_time + step10_elapsed_time + step11_elapsed_time + step12_elapsed_time + step13_elapsed_time + step14_elapsed_time + step15_elapsed_time + step16_elapsed_time + step17_elapsed_time} seconds\")\n",
    "logStep(F'06 Cached Query Reduction  : {100-(step09_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(F'14 Parquet Query Reduction : {100-(step14_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(\"END OF PROGRAM-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
