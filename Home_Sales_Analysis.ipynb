{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 22 - BIG DATA ANALYSIS WITH SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 00 - Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "notebookRunGroups": {
     "groupValue": ""
    },
    "outputId": "78c91763-b7de-43b3-ae23-1fb13e566086",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:39:57.486601\u001b[33m STEP 00 - ENVIRONMENT PREPARATION\u001b[37m----------------------------------------------------\n",
      "Copyright        : Copyright (c) 2001-2023 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved.\n",
      "OS Platform      : darwin\n",
      "OS Name          : posix\n",
      "OS HOME          : /Users/eneas\n",
      "OS uName         : Darwin\n",
      "OS NodeName      : MBPJES14M2.local\n",
      "OS Release       : 23.0.0\n",
      "OS Release Ver   : Darwin Kernel Version 23.0.0: Tue Aug 22 02:11:55 PDT 2023; root:xnu-10002.1.11~16/RELEASE_ARM64_T6020\n",
      "OS Machine       : arm64\n",
      "Process ID       : 2138\n",
      "Parent Process   : 1334\n",
      "OS User          : root\n",
      "OS User ID       : 501\n",
      "OS Group ID      : 20\n",
      "OS Effective ID  : 501\n",
      "OS Effective GID : 20\n",
      "Current dir      : /Users/eneas/Documents/GitHub/RB_Module_22_Big_Data_Challenge\n",
      "Python version   : 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:33:12) [Clang 15.0.7 ]\n",
      "Version info     : sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\n",
      "Python API Ver   : 1013\n",
      "Executable       : /Users/eneas/anaconda3/envs/myDev/bin/python\n",
      "Hadoop home      : None\n",
      "Spark version    : 2.0.1\n",
      "Spark home(Find) : /Users/eneas/anaconda3/envs/myDev/lib/python3.11/site-packages/pyspark\n",
      "Spark Home(Env)  : /Users/eneas/anaconda3/envs/myDev/lib/python3.11/site-packages/pyspark\n",
      "Spark UI         : http://localhost:4040\n",
      "Spark submit     : /Users/eneas/anaconda3/envs/myDev/lib/python3.11/site-packages/ipykernel_launcher.py\n",
      "Java home        : /Users/eneas/anaconda3/envs/myDev\n",
      "\u001b[37m2023-09-06 16:39:57.492417\u001b[33m STEP 00 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:39:57.492705\u001b[33m STEP 00 - ELAPSED TIME: 0:00:00.006233 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 00 - Environment Setup\n",
    "#\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil          as shu\n",
    "import datetime\n",
    "import warnings   \n",
    "import findspark    \n",
    "from   colorama    import Fore                       \n",
    "from   pyspark     import SparkFiles  \n",
    "from   pyspark.sql import SparkSession\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "w, h       = shu.get_terminal_size()\n",
    "\n",
    "def logStep(msg):\n",
    "    l1 = len(msg)\n",
    "    l2 = w - l1\n",
    "    print(Fore.WHITE + str(datetime.datetime.now()) +  Fore.YELLOW + ' STEP ' + msg + Fore.WHITE + '-' * l2  )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "logStep('00 - ENVIRONMENT PREPARATION')\n",
    "warnings.filterwarnings('ignore')\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def spark_SQL_Run(step_Number, step_Message1, sql_Statement):\n",
    "  start_time         = datetime.datetime.now()\n",
    "  logStep(step_Number + ' - ' + step_Message1)\n",
    "  print()\n",
    "  try:\n",
    "    spark.sql(sql_Statement).show(5)\n",
    "  except Exception as e:\n",
    "    logStep(F\"{step_Number} - Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "  else:\n",
    "    logStep(step_Number + ' - ' + 'Spark SQL Statement Executed Successfully')\n",
    "  logStep(step_Number + ' - ' + 'DONE')\n",
    "  end_time           = datetime.datetime.now()\n",
    "  step_elapsed_time  = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "def runtime_Diff(step_Number, step_Message1,base_SQL, cached_SQL):\n",
    "  start_time          = datetime.datetime.now()\n",
    "  logStep(step_Number + \" - RUNTIME DIFFERENCE\")\n",
    "  time_difference     = base_SQL - cached_SQL\n",
    "  logStep(F\"{step_Number} - Time required for a non-cached Query : {base_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time required for a cached/part Query: {cached_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time difference                      : {time_difference}\")\n",
    "  logStep(step_Number + \" - DONE\")\n",
    "  end_time            = datetime.datetime.now()\n",
    "  step_elapsed_time   = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "print(F'Copyright        : {sys.copyright}')\n",
    "print(F'OS Platform      : {sys.platform}')\n",
    "print(F'OS Name          : {os.name}')\n",
    "print(F'OS HOME          : {os.environ.get(\"HOME\")}')\n",
    "print(F'OS uName         : {os.uname().sysname}')\n",
    "print(F'OS NodeName      : {os.uname().nodename}')\n",
    "print(F'OS Release       : {os.uname().release}')\n",
    "print(F'OS Release Ver   : {os.uname().version}')\n",
    "print(F'OS Machine       : {os.uname().machine}')\n",
    "print(F'Process ID       : {os.getpid()}')\n",
    "print(F'Parent Process   : {os.getppid()}')\n",
    "print(F'OS User          : {os.getlogin()}')\n",
    "print(F'OS User ID       : {os.getuid()}')\n",
    "print(F'OS Group ID      : {os.getgid()}')\n",
    "print(F'OS Effective ID  : {os.geteuid()}')\n",
    "print(F'OS Effective GID : {os.getegid()}')\n",
    "print(F'Current dir      : {os.getcwd()}')\n",
    "print(F'Python version   : {sys.version}')\n",
    "print(F'Version info     : {sys.version_info}')\n",
    "print(F'Python API Ver   : {sys.api_version}')\n",
    "print(F'Executable       : {sys.executable}')\n",
    "print(F'Hadoop home      : {os.environ.get(\"HADOOP_HOME\")}')\n",
    "print(F'Spark version    : {findspark.__version__}')\n",
    "print(F'Spark home(Find) : {findspark.find()}')\n",
    "print(F'Spark Home(Env)  : {os.environ.get(\"SPARK_HOME\")}')\n",
    "print(F'Spark UI         : http://localhost:4040')\n",
    "print(F'Spark submit     : {sys.argv[0]}')\n",
    "print(F'Java home        : {os.environ.get(\"JAVA_HOME\")}')\n",
    "\n",
    "logStep(\"00 - DONE\");\n",
    "end_time            = datetime.datetime.now()\n",
    "step00_elapsed_time = end_time - start_time\n",
    "logStep(F\"00 - ELAPSED TIME: {step00_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01 - Read in the source file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "69591c83-18c1-4e2a-86d4-2e5219c6d006",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:17.711430\u001b[33m STEP 01 - READ SOURCE DATA\u001b[37m-----------------------------------------------------------\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:20.974130\u001b[33m STEP 01 - Spark operation - Source data read successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-06 16:36:20.974547\u001b[33m STEP 01 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:20.974849\u001b[33m STEP 01 - ELAPSED TIME: 0:00:03.263394 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 01 - Read in the source file into a DataFrame.\n",
    "#\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logStep(\"01 - READ SOURCE DATA\");\n",
    "\n",
    "try:\n",
    "\n",
    "  url   = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "  spark.sparkContext.addFile(url)\n",
    "  home_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True)\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"01 - Spark operation - Source data read successfully\")\n",
    "\n",
    "logStep(\"01 - DONE\");\n",
    "end_time           = datetime.datetime.now()\n",
    "step01_elapsed_time = end_time - start_time\n",
    "logStep(F\"01 - ELAPSED TIME: {step01_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02 - Create a temporary view of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RoljcJ7WPpnm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:20.979099\u001b[33m STEP 02 - CREATE A TEMPORARY VIEW\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:20.999770\u001b[33m STEP 02 - Spark operation - Temporary view created successfully\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:21.000157\u001b[33m STEP 02 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:21.000456\u001b[33m STEP 02 - ELAPSED TIME: 0:00:00.021336 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 02 - Create a temporary view of the DataFrame.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"02 - CREATE A TEMPORARY VIEW\")\n",
    "\n",
    "try:\n",
    "  home_df.createOrReplaceTempView('home_sales')\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"02 - Spark operation - Temporary view created successfully\")\n",
    "\n",
    "logStep(\"02 - DONE\")\n",
    "end_time           = datetime.datetime.now()\n",
    "step02_elapsed_time = end_time - start_time\n",
    "logStep(F\"02 - ELAPSED TIME: {step02_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "7d9b6b22-c722-42dc-80c0-81e8810f7521",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:21.004022\u001b[33m STEP 03 - WHAT IS THE AVERAGE PRICE?=4BR\u001b[37m---------------------------------------------\n",
      "\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2022|296363.88|\n",
      "|2021|301819.44|\n",
      "|2020|298353.78|\n",
      "|2019| 300263.7|\n",
      "+----+---------+\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:21.613914\u001b[33m STEP 03 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 16:36:21.614274\u001b[33m STEP 03 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:21.614569\u001b[33m STEP 03 - ELAPSED TIME: 0:00:00.610545 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step03_elapsed_time = spark_SQL_Run(\"03\", \"WHAT IS THE AVERAGE PRICE?=4BR\", \"SELECT YEAR(date) AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 4 GROUP BY YEAR(date) ORDER BY YEAR(date) DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "e1c36565-5164-4a9f-f36a-b2ad94522f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:21.618880\u001b[33m STEP 04 - WHAT IS THE AVERAGE PRICE?=3BR/3B\u001b[37m------------------------------------------\n",
      "\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|292676.79|\n",
      "|2016|290555.07|\n",
      "|2015| 288770.3|\n",
      "|2014|290852.27|\n",
      "|2013|295962.27|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:21.867108\u001b[33m STEP 04 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 16:36:21.867453\u001b[33m STEP 04 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:21.867664\u001b[33m STEP 04 - ELAPSED TIME: 0:00:00.248782 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step04_elapsed_time = spark_SQL_Run(\"04\", \"WHAT IS THE AVERAGE PRICE?=3BR/3B\", \"SELECT date_built AS YEAR , ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "9b51b8bc-8d1e-4a4e-ff50-f94a621891be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:21.871115\u001b[33m STEP 05 - WHAT IS THE AVERAGE PRICE?=3R/3/2B\u001b[37m-----------------------------------------\n",
      "\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|280317.58|\n",
      "|2016| 293965.1|\n",
      "|2015|297609.97|\n",
      "|2014|298264.72|\n",
      "|2013|303676.79|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:22.049191\u001b[33m STEP 05 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.049626\u001b[33m STEP 05 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.049897\u001b[33m STEP 05 - ELAPSED TIME: 0:00:00.178775 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step05_elapsed_time = spark_SQL_Run(\"05\", \"WHAT IS THE AVERAGE PRICE?=3R/3/2B\", \"SELECT date_built AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 AND floors == 2 AND sqft_living >= 2000 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "73e46678-fca9-434f-feb8-b2460e96e41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:22.054517\u001b[33m STEP 06 - WHAT IS THE AVERAGE PRICE?=300K\u001b[37m--------------------------------------------\n",
      "\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:22.249768\u001b[33m STEP 06 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.250135\u001b[33m STEP 06 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.250376\u001b[33m STEP 06 - ELAPSED TIME: 0:00:00.195854 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Ste 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "#\n",
    "\n",
    "step06_elapsed_time = spark_SQL_Run(\"06\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 07 - Cache the the temporary table home_sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "7a421ad4-699e-4ae1-c13e-3ac1379a3cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:22.255095\u001b[33m STEP 07 - CACHE HOME DATA\u001b[37m------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.596281\u001b[33m STEP 07 - Spark operation - Cache executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.596750\u001b[33m STEP 07 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.597067\u001b[33m STEP 07 - ELAPSED TIME: 0:00:00.341649 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 07 - Cache the the temporary table home_sales.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"07 - CACHE HOME DATA\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"cache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1) \n",
    "else:\n",
    "  logStep(\"07 - Spark operation - Cache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"07 - DONE\")\n",
    "step07_elapsed_time = end_time - start_time\n",
    "logStep(F\"07 - ELAPSED TIME: {step07_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 08 - Check if the table is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "3003ef79-0678-4a80-e79a-eac9fdfc8a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:22.600958\u001b[33m STEP 08 - IS THE DATA CACHED?\u001b[37m--------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.609834\u001b[33m STEP 08 - home_sales is cached\u001b[37m-------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.610420\u001b[33m STEP 08 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.611212\u001b[33m STEP 08 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.612207\u001b[33m STEP 08 - ELAPSED TIME: 0:00:00.010212 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 08 - Check if the table is cached.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"08 - IS THE DATA CACHED?\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "    logStep(\"08 - home_sales is not cached\")\n",
    "  else:\n",
    "    logStep(\"08 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "    logStep(F\"Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logStep(\"08 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"08 - DONE\")\n",
    "step08_elapsed_time = end_time - start_time\n",
    "logStep(F\"08 - ELAPSED TIME: {step08_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "a7aec34a-063b-46bc-b428-8660a32d9a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:22.621377\u001b[33m STEP 09 - WHAT IS THE AVERAGE PRICE?=300K\u001b[37m--------------------------------------------\n",
      "\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:22.736988\u001b[33m STEP 09 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.737373\u001b[33m STEP 09 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.737580\u001b[33m STEP 09 - ELAPSED TIME: 0:00:00.116203 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "#\n",
    "\n",
    "step09_elapsed_time = spark_SQL_Run(\"09\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAYsuFaZuCmw",
    "outputId": "c5787884-c516-4f61-9956-6edfc0dcbf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:22.741197\u001b[33m STEP 10 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.741768\u001b[33m STEP 10 - Time required for a non-cached Query : 0:00:00.195854\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:22.741971\u001b[33m STEP 10 - Time required for a cached/part Query: 0:00:00.116203\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:22.742166\u001b[33m STEP 10 - Time difference                      : 0:00:00.079651\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:22.742333\u001b[33m STEP 10 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:22.742501\u001b[33m STEP 10 - ELAPSED TIME: 0:00:00.001303 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 10 Determine the runtime and compare to the original runtime\n",
    "#\n",
    "\n",
    "step10_elapsed_time = runtime_Diff(\"10\", \"RUNTIME DIFFERENCE\", step06_elapsed_time, step09_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:22.746573\u001b[33m STEP 11 - FORMATTED PARQUET\u001b[37m----------------------------------------------------------\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:23.571055\u001b[33m STEP 11 - Spark operation - Parquet file created successfully.\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-06 16:36:23.571492\u001b[33m STEP 11 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.571783\u001b[33m STEP 11 - ELAPSED TIME: 0:00:00.824915 seconds\u001b[37m---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"11 - FORMATTED PARQUET\")\n",
    "\n",
    "try:\n",
    "  home_df.write.parquet('home_parquet', mode='overwrite',partitionBy='date_built')\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"11 - Spark operation - Parquet file created successfully.\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"11 - DONE\")\n",
    "step11_elapsed_time = end_time - start_time\n",
    "logStep(F\"11 - ELAPSED TIME: {step11_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12 - Read the parquet formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.575780\u001b[33m STEP 12 - READ PARQUET\u001b[37m---------------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:23.749558\u001b[33m STEP 12 - Spark operation - Parquet file read successfully\u001b[37m---------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.749918\u001b[33m STEP 12 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.750174\u001b[33m STEP 12 - ELAPSED TIME: 0:00:00.174136 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 12 - Read the parquet formatted data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"12 - READ PARQUET\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df = spark.read.parquet('home_parquet')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"12 - Spark operation - Parquet file read successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"12 - DONE\")\n",
    "step12_elapsed_time = end_time - start_time\n",
    "logStep(F\"12 - ELAPSED TIME: {step12_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13 - Create a temporary table for the parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.754126\u001b[33m STEP 13 - CREATE PARQUET VIEW\u001b[37m--------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:23.791667\u001b[33m STEP 13 - Spark operation - Parquet view created successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.792070\u001b[33m STEP 13 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.792327\u001b[33m STEP 13 - ELAPSED TIME: 0:00:00.037941 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 13 - Create a temporary table for the parquet data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"13 - CREATE PARQUET VIEW\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df.createOrReplaceTempView('parquet_temp_home')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  print(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"13 - Spark operation - Parquet view created successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"13 - DONE\")\n",
    "step13_elapsed_time = end_time - start_time\n",
    "logStep(F\"13 - ELAPSED TIME: {step13_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "5420b7c8-7f4b-404f-fc91-f70a06039866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.795770\u001b[33m STEP 14 - 14 - REPEAT QUERY\u001b[37m----------------------------------------------------------\n",
      "\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 16:36:23.955397\u001b[33m STEP 14 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.955697\u001b[33m STEP 14 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.955882\u001b[33m STEP 14 - ELAPSED TIME: 0:00:00.160110 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places. \n",
    "#\n",
    "\n",
    "step14_elapsed_time = spark_SQL_Run(\"14\", \"14 - REPEAT QUERY\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM parquet_temp_home GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 15 Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEkIu5yYuWlW",
    "outputId": "db861ec0-49c1-4855-86b4-f360d566302e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.959657\u001b[33m STEP 15 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.960018\u001b[33m STEP 15 - Time required for a non-cached Query : 0:00:00.195854\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:23.960232\u001b[33m STEP 15 - Time required for a cached/part Query: 0:00:00.160110\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:23.960412\u001b[33m STEP 15 - Time difference                      : 0:00:00.035744\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 16:36:23.960592\u001b[33m STEP 15 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.960782\u001b[33m STEP 15 - ELAPSED TIME: 0:00:00.001127 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 15 - Determine the runtime and compare it to the cached version.\n",
    "#\n",
    "\n",
    "step15_elapsed_time = runtime_Diff(\"15\", \"RUNTIME DIFFERENCE\", step06_elapsed_time, step14_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 16 - Un-cache the home_sales temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "5b8e7eaf-78eb-4eef-d29d-1be425668fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.964861\u001b[33m STEP 16 - UNCACHE\u001b[37m--------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.977473\u001b[33m STEP 16 - Spark operation - Uncache executed successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.978197\u001b[33m STEP 16 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.978512\u001b[33m STEP 16 - ELAPSED TIME: 0:00:00.013329 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 16 - Un-cache the home_sales temporary table.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"16 - UNCACHE\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"uncache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"16 - Spark operation - Uncache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"16 - DONE\")\n",
    "step16_elapsed_time = end_time - start_time\n",
    "logStep(F\"16 - ELAPSED TIME: {step16_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 17 - Check if the home_sales is no longer cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "a3d33ffb-4052-41c6-8040-bff5a22a2188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.982294\u001b[33m STEP 17 - CACHE CHECK\u001b[37m----------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.986712\u001b[33m STEP 17 - home_sales is not cached\u001b[37m---------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.988635\u001b[33m STEP 17 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.989372\u001b[33m STEP 17 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:23.990092\u001b[33m STEP 17 - ELAPSED TIME: 0:00:00.007076 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 17 - Check if the home_sales is no longer cached\n",
    "#\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"17 - CACHE CHECK\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "      logStep(\"17 - home_sales is not cached\")\n",
    "  else:\n",
    "      logStep(\"17 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"17 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"17 - DONE\")\n",
    "step17_elapsed_time = end_time - start_time\n",
    "logStep(F\"17 - ELAPSED TIME: {step17_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 16:36:23.999642\u001b[33m STEP TOTAL RUN TIME\u001b[37m------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.001315\u001b[33m STEP  0:   0:00:02.396406 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.002319\u001b[33m STEP  1:   0:00:03.263394 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.002630\u001b[33m STEP  2:   0:00:00.021336 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.003109\u001b[33m STEP  3:   0:00:00.610545 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.004342\u001b[33m STEP  4:   0:00:00.248782 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.005067\u001b[33m STEP  5:   0:00:00.178775 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.005778\u001b[33m STEP  6:   0:00:00.195854 seconds - non-cached query\u001b[37m---------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.006213\u001b[33m STEP  7:   0:00:00.341649 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.006520\u001b[33m STEP  8:   0:00:00.010212 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.006767\u001b[33m STEP  9:   0:00:00.116203 seconds - cached query\u001b[37m-------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.007023\u001b[33m STEP 10:   0:00:00.001303 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.007236\u001b[33m STEP 11:   0:00:00.824915 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.007437\u001b[33m STEP 12:   0:00:00.174136 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.007715\u001b[33m STEP 13:   0:00:00.037941 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.008017\u001b[33m STEP 14:   0:00:00.160110 seconds - parquet cached query\u001b[37m-----------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.008201\u001b[33m STEP 15:   0:00:00.001127 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.008403\u001b[33m STEP 16:   0:00:00.013329 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.008650\u001b[33m STEP 17:   0:00:00.007076 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.008876\u001b[33m STEP TT:   0:00:08.603093 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.009084\u001b[33m STEP 06 Cached Query Reduction  : 40.67%\u001b[37m---------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.009314\u001b[33m STEP 14 Parquet Query Reduction : 18.25%\u001b[37m---------------------------------------------\n",
      "\u001b[37m2023-09-06 16:36:24.009512\u001b[33m STEP END OF PROGRAM-\u001b[37m-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "logStep(\"TOTAL RUN TIME\")\n",
    "logStep(F\" 0:   {step00_elapsed_time} seconds\")\n",
    "logStep(F\" 1:   {step01_elapsed_time} seconds\")\n",
    "logStep(F\" 2:   {step02_elapsed_time} seconds\")\n",
    "logStep(F\" 3:   {step03_elapsed_time} seconds\")\n",
    "logStep(F\" 4:   {step04_elapsed_time} seconds\")\n",
    "logStep(F\" 5:   {step05_elapsed_time} seconds\")\n",
    "logStep(F\" 6:   {step06_elapsed_time} seconds - non-cached query\")\n",
    "logStep(F\" 7:   {step07_elapsed_time} seconds\")\n",
    "logStep(F\" 8:   {step08_elapsed_time} seconds\")\n",
    "logStep(F\" 9:   {step09_elapsed_time} seconds - cached query\")\n",
    "logStep(F\"10:   {step10_elapsed_time} seconds\")\n",
    "logStep(F\"11:   {step11_elapsed_time} seconds\")\n",
    "logStep(F\"12:   {step12_elapsed_time} seconds\")\n",
    "logStep(F\"13:   {step13_elapsed_time} seconds\")\n",
    "logStep(F\"14:   {step14_elapsed_time} seconds - parquet cached query\")\n",
    "logStep(F\"15:   {step15_elapsed_time} seconds\")\n",
    "logStep(F\"16:   {step16_elapsed_time} seconds\")\n",
    "logStep(F\"17:   {step17_elapsed_time} seconds\")\n",
    "logStep(F\"TT:   {step00_elapsed_time + step01_elapsed_time + step02_elapsed_time + step03_elapsed_time + step04_elapsed_time + step05_elapsed_time + step06_elapsed_time + step07_elapsed_time + step08_elapsed_time + step09_elapsed_time + step10_elapsed_time + step11_elapsed_time + step12_elapsed_time + step13_elapsed_time + step14_elapsed_time + step15_elapsed_time + step16_elapsed_time + step17_elapsed_time} seconds\")\n",
    "logStep(F'06 Cached Query Reduction  : {100-(step09_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(F'14 Parquet Query Reduction : {100-(step14_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(\"END OF PROGRAM-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
