{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 22 - BIG DATA ANALYSIS WITH SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 00 - Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "notebookRunGroups": {
     "groupValue": ""
    },
    "outputId": "78c91763-b7de-43b3-ae23-1fb13e566086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:30.536457\u001b[33m STEP 00 - ENVIRONMENT PREPARATION\u001b[37m----------------------------------------------------\n",
      "Copyright        : Copyright (c) 2001-2023 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved.\n",
      "OS Platform      : darwin\n",
      "OS Name          : posix\n",
      "OS HOME          : /Users/eneas\n",
      "OS uName         : Darwin\n",
      "OS NodeName      : MBPJES14M2.local\n",
      "OS Release       : 23.0.0\n",
      "OS Release Ver   : Darwin Kernel Version 23.0.0: Tue Aug 22 02:11:55 PDT 2023; root:xnu-10002.1.11~16/RELEASE_ARM64_T6020\n",
      "OS Machine       : arm64\n",
      "Process ID       : 16469\n",
      "Parent Process   : 8563\n",
      "OS User          : root\n",
      "OS User ID       : 501\n",
      "OS Group ID      : 20\n",
      "OS Effective ID  : 501\n",
      "OS Effective GID : 20\n",
      "Current dir      : /Users/eneas/Documents/GitHub/RB_Module_22_Big_Data_Challenge\n",
      "Python version   : 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:33:12) [Clang 15.0.7 ]\n",
      "Version info     : sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\n",
      "Python API Ver   : 1013\n",
      "Executable       : /Users/eneas/anaconda3/envs/myProd/bin/python\n",
      "Hadoop home      : None\n",
      "Spark version    : 2.0.1\n",
      "Spark home(Find) : /Users/eneas/anaconda3/envs/myProd/lib/python3.11/site-packages/pyspark\n",
      "Spark Home(Env)  : None\n",
      "Spark UI         : http://localhost:4040\n",
      "Spark submit     : /Users/eneas/anaconda3/envs/myProd/lib/python3.11/site-packages/ipykernel_launcher.py\n",
      "Java home        : /Users/eneas/anaconda3/envs/myProd\n",
      "\u001b[37m2023-09-05 18:19:30.608940\u001b[33m STEP 00 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:30.624270\u001b[33m STEP 00 - ELAPSED TIME: 0:00:00.087831 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 00 - Environment Setup\n",
    "#\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil          as shu\n",
    "import datetime\n",
    "import warnings   \n",
    "import findspark    \n",
    "from   colorama    import Fore                       \n",
    "from   pyspark     import SparkFiles  \n",
    "from   pyspark.sql import SparkSession\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "w, h       = shu.get_terminal_size()\n",
    "\n",
    "def logStep(msg):\n",
    "    l1 = len(msg)\n",
    "    l2 = w - l1\n",
    "    print(Fore.WHITE + str(datetime.datetime.now()) +  Fore.YELLOW + ' STEP ' + msg + Fore.WHITE + '-' * l2  )\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def printSeparator():\n",
    "    print(Fore.GREEN + '-' * w + Fore.WHITE)\n",
    "    \n",
    "logStep('00 - ENVIRONMENT PREPARATION')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(F'Copyright        : {sys.copyright}')\n",
    "print(F'OS Platform      : {sys.platform}')\n",
    "print(F'OS Name          : {os.name}')\n",
    "print(F'OS HOME          : {os.environ.get(\"HOME\")}')\n",
    "print(F'OS uName         : {os.uname().sysname}')\n",
    "print(F'OS NodeName      : {os.uname().nodename}')\n",
    "print(F'OS Release       : {os.uname().release}')\n",
    "print(F'OS Release Ver   : {os.uname().version}')\n",
    "print(F'OS Machine       : {os.uname().machine}')\n",
    "print(F'Process ID       : {os.getpid()}')\n",
    "print(F'Parent Process   : {os.getppid()}')\n",
    "print(F'OS User          : {os.getlogin()}')\n",
    "print(F'OS User ID       : {os.getuid()}')\n",
    "print(F'OS Group ID      : {os.getgid()}')\n",
    "print(F'OS Effective ID  : {os.geteuid()}')\n",
    "print(F'OS Effective GID : {os.getegid()}')\n",
    "print(F'Current dir      : {os.getcwd()}')\n",
    "print(F'Python version   : {sys.version}')\n",
    "print(F'Version info     : {sys.version_info}')\n",
    "print(F'Python API Ver   : {sys.api_version}')\n",
    "print(F'Executable       : {sys.executable}')\n",
    "print(F'Hadoop home      : {os.environ.get(\"HADOOP_HOME\")}')\n",
    "print(F'Spark version    : {findspark.__version__}')\n",
    "print(F'Spark home(Find) : {findspark.find()}')\n",
    "print(F'Spark Home(Env)  : {os.environ.get(\"SPARK_HOME\")}')\n",
    "print(F'Spark UI         : http://localhost:4040')\n",
    "print(F'Spark submit     : {sys.argv[0]}')\n",
    "print(F'Java home        : {os.environ.get(\"JAVA_HOME\")}')\n",
    "\n",
    "logStep(\"00 - DONE\");\n",
    "end_time           = datetime.datetime.now()\n",
    "step00_elapsed_time = end_time - start_time\n",
    "logStep(F\"00 - ELAPSED TIME: {step00_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01 - Read in the source file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "69591c83-18c1-4e2a-86d4-2e5219c6d006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:30.646601\u001b[33m STEP 01 - READ SOURCE DATA\u001b[37m-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/05 18:19:31 WARN Utils: Your hostname, MBPJES14M2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.237 instead (on interface en0)\n",
      "23/09/05 18:19:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/05 18:19:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:37.995885\u001b[33m STEP 01 - Spark operation - Source data read successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-05 18:19:37.996364\u001b[33m STEP 01 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:37.996724\u001b[33m STEP 01 - ELAPSED TIME: 0:00:07.350099 seconds\u001b[37m---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 01 - Read in the source file into a DataFrame.\n",
    "#\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logStep(\"01 - READ SOURCE DATA\");\n",
    "\n",
    "try:\n",
    "  findspark.init()\n",
    "  spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "  spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  url   = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "  spark.sparkContext.addFile(url)\n",
    "  home_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True)\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"01 - Spark operation - Source data read successfully\")\n",
    "\n",
    "logStep(\"01 - DONE\");\n",
    "end_time           = datetime.datetime.now()\n",
    "step01_elapsed_time = end_time - start_time\n",
    "logStep(F\"01 - ELAPSED TIME: {step01_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02 - Create a temporary view of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RoljcJ7WPpnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:38.001958\u001b[33m STEP 02 - CREATE A TEMPORARY VIEW\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:38.020744\u001b[33m STEP 02 - Spark operation - Temporary view created successfully\u001b[37m----------------------\n",
      "\u001b[37m2023-09-05 18:19:38.021235\u001b[33m STEP 02 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:38.021562\u001b[33m STEP 02 - ELAPSED TIME: 0:00:00.019586 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 02 - Create a temporary view of the DataFrame.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"02 - CREATE A TEMPORARY VIEW\")\n",
    "\n",
    "try:\n",
    "  home_df.createOrReplaceTempView('home_sales')\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"02 - Spark operation - Temporary view created successfully\")\n",
    "\n",
    "logStep(\"02 - DONE\")\n",
    "end_time           = datetime.datetime.now()\n",
    "step02_elapsed_time = end_time - start_time\n",
    "logStep(F\"02 - ELAPSED TIME: {step02_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "7d9b6b22-c722-42dc-80c0-81e8810f7521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:38.025677\u001b[33m STEP 03 - WHAT IS THE AVERAGE PRICE?=4BR\u001b[37m---------------------------------------------\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2022|296363.88|\n",
      "|2021|301819.44|\n",
      "|2020|298353.78|\n",
      "|2019| 300263.7|\n",
      "+----+---------+\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:38.655980\u001b[33m STEP 03 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-05 18:19:38.656408\u001b[33m STEP 03 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:38.656850\u001b[33m STEP 03 - ELAPSED TIME: 0:00:00.631144 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"03 - WHAT IS THE AVERAGE PRICE?=4BR\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"SELECT YEAR(date)          AS YEAR, \\\n",
    "                    ROUND(AVG(price),2) AS AVERAGE \\\n",
    "               FROM home_sales \\\n",
    "              WHERE bedrooms == 4 \\\n",
    "           GROUP BY YEAR(date) \\\n",
    "           ORDER BY YEAR(date) DESC\").show(5)\n",
    "except Exception as e:\n",
    "     logStep(F\"Exception: {e}\")\n",
    "     sys.exit(1)\n",
    "else:\n",
    "     logStep(\"03 - Spark operation - Query executed successfully\")\n",
    "\n",
    "     \n",
    "logStep(\"03 - DONE\")\n",
    "end_time           = datetime.datetime.now()\n",
    "step03_elapsed_time = end_time - start_time\n",
    "logStep(F\"03 - ELAPSED TIME: {step03_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "e1c36565-5164-4a9f-f36a-b2ad94522f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:38.661680\u001b[33m STEP 04 - WHAT IS THE AVERAGE PRICE?=3BR/3B\u001b[37m------------------------------------------\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|292676.79|\n",
      "|2016|290555.07|\n",
      "|2015| 288770.3|\n",
      "|2014|290852.27|\n",
      "|2013|295962.27|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:38.886209\u001b[33m STEP 04 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-05 18:19:38.886687\u001b[33m STEP 04 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:38.887078\u001b[33m STEP 04 - ELAPSED TIME: 0:00:00.225002 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"04 - WHAT IS THE AVERAGE PRICE?=3BR/3B\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"SELECT date_built          AS YEAR , \\\n",
    "                    ROUND(AVG(price),2) AS AVERAGE  \\\n",
    "               FROM home_sales \\\n",
    "              WHERE bedrooms == 3 AND \\\n",
    "                    bathrooms == 3 \\\n",
    "          GROUP BY date_built \\\n",
    "          ORDER BY date_built DESC\").show(5)    \n",
    "except Exception as e:\n",
    "     logStep(F\"Exception: {e}\")\n",
    "     sys.exit(1)\n",
    "else:\n",
    "     logStep(\"04 - Spark operation - Query executed successfully\")\n",
    "\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"04 - DONE\")\n",
    "step04_elapsed_time = end_time - start_time\n",
    "logStep(F\"04 - ELAPSED TIME: {step04_elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "9b51b8bc-8d1e-4a4e-ff50-f94a621891be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:38.891967\u001b[33m STEP 05 - WHAT IS THE AVERAGE PRICE?=3R/3/2\u001b[37m------------------------------------------\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|280317.58|\n",
      "|2016| 293965.1|\n",
      "|2015|297609.97|\n",
      "|2014|298264.72|\n",
      "|2013|303676.79|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:39.056596\u001b[33m STEP 05 = Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.057047\u001b[33m STEP 05 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.057424\u001b[33m STEP 05 - ELAPSED TIME: 0:00:00.165073 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"05 - WHAT IS THE AVERAGE PRICE?=3R/3/2\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"SELECT date_built          AS YEAR, \\\n",
    "                    ROUND(AVG(price),2) AS AVERAGE \\\n",
    "               FROM home_sales \\\n",
    "              WHERE bedrooms    == 3 AND \\\n",
    "                    bathrooms   == 3 AND \\\n",
    "                    floors      == 2 AND \\\n",
    "                    sqft_living >= 2000 \\\n",
    "           GROUP BY date_built \\\n",
    "           ORDER BY date_built DESC\").show(5)\n",
    "except Exception as e:\n",
    "  LogStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"05 = Spark operation - Query executed successfully\")\n",
    "\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"05 - DONE\")\n",
    "step05_elapsed_time = end_time - start_time\n",
    "logStep(F\"05 - ELAPSED TIME: {step05_elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "73e46678-fca9-434f-feb8-b2460e96e41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:39.063872\u001b[33m STEP 06 - WHAT IS THE AVERAGE PRICE?=300K\u001b[37m--------------------------------------------\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:39.243003\u001b[33m STEP 06 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.243481\u001b[33m STEP 06 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.243827\u001b[33m STEP 06 - ELAPSED TIME: 0:00:00.179600 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Ste 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"06 - WHAT IS THE AVERAGE PRICE?=300K\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"SELECT view                AS VIEW, \\\n",
    "                    ROUND(AVG(price),2) AS AVERAGE \\\n",
    "               FROM home_sales \\\n",
    "           GROUP BY view \\\n",
    "             HAVING ROUND(AVG(price),2) >= 350000 \\\n",
    "           ORDER BY view DESC\").show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1) \n",
    "else:\n",
    "  logStep(\"06 - Spark operation - Query executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"06 - DONE\")\n",
    "step06_elapsed_time = end_time - start_time\n",
    "logStep(F\"06 - ELAPSED TIME: {step06_elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 07 - Cache the the temporary table home_sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "7a421ad4-699e-4ae1-c13e-3ac1379a3cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:39.248223\u001b[33m STEP 07 - CACHE HOME DATA\u001b[37m------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.592877\u001b[33m STEP 07 - Spark operation - Cache executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.593435\u001b[33m STEP 07 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.593792\u001b[33m STEP 07 - ELAPSED TIME: 0:00:00.345205 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 07 - Cache the the temporary table home_sales.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"07 - CACHE HOME DATA\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"cache table home_sales\")\n",
    "except Exception as e:\n",
    "    logStep(F\"Exception: {e}\")\n",
    "    sys.exit(1) \n",
    "else:\n",
    "    logStep(\"07 - Spark operation - Cache executed successfully\")\n",
    "\n",
    "    \n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"07 - DONE\")\n",
    "step07_elapsed_time = end_time - start_time\n",
    "logStep(F\"07 - ELAPSED TIME: {step07_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 08 - Check if the table is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "3003ef79-0678-4a80-e79a-eac9fdfc8a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:39.597865\u001b[33m STEP 08 - IS THE DATA CACHED?\u001b[37m--------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.604982\u001b[33m STEP 08 - home_sales is cached\u001b[37m-------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.605355\u001b[33m STEP 08 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.605727\u001b[33m STEP 08 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.606047\u001b[33m STEP 08 - ELAPSED TIME: 0:00:00.007857 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 08 - Check if the table is cached.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"08 - IS THE DATA CACHED?\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "    logStep(\"08 - home_sales is not cached\")\n",
    "  else:\n",
    "    logStep(\"08 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "    logStep(F\"Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logStep(\"08 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"08 - DONE\")\n",
    "step08_elapsed_time = end_time - start_time\n",
    "logStep(F\"08 - ELAPSED TIME: {step08_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "a7aec34a-063b-46bc-b428-8660a32d9a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:39.610097\u001b[33m STEP 09 - REPEAT QUERY\u001b[37m---------------------------------------------------------------\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:39.714487\u001b[33m STEP 09 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.715201\u001b[33m STEP 09 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.715577\u001b[33m STEP 09 - ELAPSED TIME: 0:00:00.105099 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"09 - REPEAT QUERY\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"SELECT view                AS VIEW, \\\n",
    "                    ROUND(AVG(price),2) AS AVERAGE \\\n",
    "               FROM home_sales \\\n",
    "           GROUP BY view \\\n",
    "             HAVING ROUND(AVG(price),2) >= 350000 \\\n",
    "           ORDER BY view DESC\").show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"09 - Spark operation - Query executed successfully\")\n",
    "  \n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"09 - DONE\")\n",
    "step09_elapsed_time = end_time - start_time\n",
    "logStep(F\"09 - ELAPSED TIME: {step09_elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAYsuFaZuCmw",
    "outputId": "c5787884-c516-4f61-9956-6edfc0dcbf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:39.719578\u001b[33m STEP 10 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.720164\u001b[33m STEP 10 - Time required for a non-cached Query: 0:00:00.179600\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-05 18:19:39.720695\u001b[33m STEP 10 - Time required for a cached Query    : 0:00:00.105099\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-05 18:19:39.721030\u001b[33m STEP 10 - Time difference                     : 0:00:00.074501\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-05 18:19:39.721452\u001b[33m STEP 10 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:39.721627\u001b[33m STEP 10 - ELAPSED TIME: 0:00:00.001876 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 10 Determine the runtime and compare to the original runtime\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"10 - RUNTIME DIFFERENCE\")\n",
    "\n",
    "time_difference = step06_elapsed_time - step09_elapsed_time\n",
    "logStep(F\"10 - Time required for a non-cached Query: {step06_elapsed_time}\")\n",
    "logStep(F\"10 - Time required for a cached Query    : {step09_elapsed_time}\")\n",
    "logStep(F\"10 - Time difference                     : {time_difference}\")\n",
    "\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"10 - DONE\")\n",
    "step10_elapsed_time = end_time - start_time\n",
    "logStep(F\"10 - ELAPSED TIME: {step10_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:39.726560\u001b[33m STEP 11 - FORMATTED PARQUET\u001b[37m----------------------------------------------------------\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:40.589370\u001b[33m STEP 11 - Spark operation - Parquet file created successfully.\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-05 18:19:40.589834\u001b[33m STEP 11 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.590131\u001b[33m STEP 11 - ELAPSED TIME: 0:00:00.863267 seconds\u001b[37m---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"11 - FORMATTED PARQUET\")\n",
    "\n",
    "try:\n",
    "  home_df.write.parquet('home_parquet', mode='overwrite',partitionBy='date_built')\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"11 - Spark operation - Parquet file created successfully.\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"11 - DONE\")\n",
    "step11_elapsed_time = end_time - start_time\n",
    "logStep(F\"11 - ELAPSED TIME: {step11_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12 - Read the parquet formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:40.594072\u001b[33m STEP 12 - READ PARQUET\u001b[37m---------------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:40.765917\u001b[33m STEP 12 - Spark operation - Parquet file read successfully\u001b[37m---------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.766360\u001b[33m STEP 12 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.766713\u001b[33m STEP 12 - ELAPSED TIME: 0:00:00.172284 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 12 - Read the parquet formatted data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"12 - READ PARQUET\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df = spark.read.parquet('home_parquet')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(Fore.RED + F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"12 - Spark operation - Parquet file read successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"12 - DONE\")\n",
    "step12_elapsed_time = end_time - start_time\n",
    "logStep(F\"12 - ELAPSED TIME: {step12_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13 - Create a temporary table for the parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:40.771020\u001b[33m STEP 13 - CREATE PARQUET VIEW\u001b[37m--------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:40.808771\u001b[33m STEP 13 - Spark operation - Parquet view created successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.809178\u001b[33m STEP 13 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.809539\u001b[33m STEP 13 - ELAPSED TIME: 0:00:00.038152 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 13 - Create a temporary table for the parquet data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"13 - CREATE PARQUET VIEW\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df.createOrReplaceTempView('parquet_temp_home')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  print(Fore.RED + F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"13 - Spark operation - Parquet view created successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"13 - DONE\")\n",
    "step13_elapsed_time = end_time - start_time\n",
    "logStep(F\"13 - ELAPSED TIME: {step13_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "5420b7c8-7f4b-404f-fc91-f70a06039866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:40.814094\u001b[33m STEP 14 - REPEAT QUERY\u001b[37m---------------------------------------------------------------\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-05 18:19:40.986917\u001b[33m STEP 14 - Spark operation - Parquet query executed successfully.\u001b[37m---------------------\n",
      "\u001b[37m2023-09-05 18:19:40.987383\u001b[33m STEP 14 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.987881\u001b[33m STEP 14 - ELAPSED TIME: 0:00:00.173286 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places. \n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"14 - REPEAT QUERY\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"SELECT view                AS VIEW, \\\n",
    "                    ROUND(AVG(price),2) AS AVERAGE \\\n",
    "               FROM parquet_temp_home \\\n",
    "           GROUP BY view \\\n",
    "             HAVING ROUND(AVG(price),2) >= 350000 \\\n",
    "           ORDER BY view DESC\").show(5)\n",
    "except Exception as e:\n",
    "  logStep(Fore.RED + F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"14 - Spark operation - Parquet query executed successfully.\")\n",
    "  \n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"14 - DONE\")\n",
    "step14_elapsed_time = end_time - start_time\n",
    "logStep(F\"14 - ELAPSED TIME: {step14_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 15 Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEkIu5yYuWlW",
    "outputId": "db861ec0-49c1-4855-86b4-f360d566302e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:40.992068\u001b[33m STEP 15 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.992925\u001b[33m STEP 15 - Time required for a non-cached Query    : 0:00:00.179600\u001b[37m-------------------\n",
      "\u001b[37m2023-09-05 18:19:40.993290\u001b[33m STEP 15 - Time required for a parquet cached Query: 0:00:00.173286\u001b[37m-------------------\n",
      "\u001b[37m2023-09-05 18:19:40.993864\u001b[33m STEP 15 - Time difference                         : 0:00:00.006314\u001b[37m-------------------\n",
      "\u001b[37m2023-09-05 18:19:40.994100\u001b[33m STEP 15 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:40.994392\u001b[33m STEP 15 - ELAPSED TIME: 0:00:00.002309 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 15 - Determine the runtime and compare it to the cached version.\n",
    "#\n",
    "\n",
    "start_time          = datetime.datetime.now()\n",
    "logStep(\"15 - RUNTIME DIFFERENCE\")\n",
    "\n",
    "time_difference     = step06_elapsed_time - step14_elapsed_time\n",
    "logStep(F\"15 - Time required for a non-cached Query    : {step06_elapsed_time}\")\n",
    "logStep(F\"15 - Time required for a parquet cached Query: {step14_elapsed_time}\")\n",
    "logStep(F\"15 - Time difference                         : {time_difference}\")\n",
    "logStep(\"15 - DONE\")\n",
    "end_time            = datetime.datetime.now()\n",
    "step15_elapsed_time = end_time - start_time\n",
    "logStep(F\"15 - ELAPSED TIME: {step15_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 16 - Un-cache the home_sales temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "5b8e7eaf-78eb-4eef-d29d-1be425668fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:40.998399\u001b[33m STEP 16 - UNCACHE\u001b[37m--------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.010222\u001b[33m STEP 16 - Spark operation - Uncache executed successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.010662\u001b[33m STEP 16 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.010978\u001b[33m STEP 16 - ELAPSED TIME: 0:00:00.012256 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 16 - Un-cache the home_sales temporary table.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"16 - UNCACHE\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"uncache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(Fore.RED + F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"16 - Spark operation - Uncache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"16 - DONE\")\n",
    "step16_elapsed_time = end_time - start_time\n",
    "logStep(F\"16 - ELAPSED TIME: {step16_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 17 - Check if the home_sales is no longer cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "a3d33ffb-4052-41c6-8040-bff5a22a2188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:41.020952\u001b[33m STEP 17 - CACHE CHECK\u001b[37m----------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.025353\u001b[33m STEP 17 - home_sales is not cached\u001b[37m---------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.027663\u001b[33m STEP 17 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.028416\u001b[33m STEP 17 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.030982\u001b[33m STEP 17 - ELAPSED TIME: 0:00:00.007460 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 17 - Check if the home_sales is no longer cached\n",
    "#\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"17 - CACHE CHECK\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "      logStep(\"17 - home_sales is not cached\")\n",
    "  else:\n",
    "      logStep(\"17 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"17 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"17 - DONE\")\n",
    "step17_elapsed_time = end_time - start_time\n",
    "logStep(F\"17 - ELAPSED TIME: {step17_elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-05 18:19:41.045357\u001b[33m STEP TOTAL RUN TIME\u001b[37m------------------------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.046834\u001b[33m STEP  0:   0:00:00.087831 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.047971\u001b[33m STEP  1:   0:00:07.350099 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.048960\u001b[33m STEP  2:   0:00:00.019586 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.050152\u001b[33m STEP  3:   0:00:00.631144 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.051906\u001b[33m STEP  4:   0:00:00.225002 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.052686\u001b[33m STEP  5:   0:00:00.165073 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.053382\u001b[33m STEP  6:   0:00:00.179600 seconds - non-cached query\u001b[37m---------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.054420\u001b[33m STEP  7:   0:00:00.345205 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.055003\u001b[33m STEP  8:   0:00:00.007857 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.056281\u001b[33m STEP  9:   0:00:00.105099 seconds - cached query\u001b[37m-------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.057651\u001b[33m STEP 10:   0:00:00.001876 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.058324\u001b[33m STEP 11:   0:00:00.863267 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.058742\u001b[33m STEP 12:   0:00:00.172284 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.059145\u001b[33m STEP 13:   0:00:00.038152 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.059609\u001b[33m STEP 14:   0:00:00.173286 seconds - parquet cached query\u001b[37m-----------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.059983\u001b[33m STEP 15:   0:00:00.002309 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.060444\u001b[33m STEP 16:   0:00:00.012256 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.060882\u001b[33m STEP 17:   0:00:00.007460 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.061535\u001b[33m STEP TT:   0:00:10.387386 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.062012\u001b[33m STEP 06 Cached Query Reduction  : 41.48%\u001b[37m---------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.062457\u001b[33m STEP 14 Parquet Query Reduction : 3.52%\u001b[37m----------------------------------------------\n",
      "\u001b[37m2023-09-05 18:19:41.062769\u001b[33m STEP END OF PROGRAM-\u001b[37m-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "logStep(\"TOTAL RUN TIME\")\n",
    "logStep(F\" 0:   {step00_elapsed_time} seconds\")\n",
    "logStep(F\" 1:   {step01_elapsed_time} seconds\")\n",
    "logStep(F\" 2:   {step02_elapsed_time} seconds\")\n",
    "logStep(F\" 3:   {step03_elapsed_time} seconds\")\n",
    "logStep(F\" 4:   {step04_elapsed_time} seconds\")\n",
    "logStep(F\" 5:   {step05_elapsed_time} seconds\")\n",
    "logStep(F\" 6:   {step06_elapsed_time} seconds - non-cached query\")\n",
    "logStep(F\" 7:   {step07_elapsed_time} seconds\")\n",
    "logStep(F\" 8:   {step08_elapsed_time} seconds\")\n",
    "logStep(F\" 9:   {step09_elapsed_time} seconds - cached query\")\n",
    "logStep(F\"10:   {step10_elapsed_time} seconds\")\n",
    "logStep(F\"11:   {step11_elapsed_time} seconds\")\n",
    "logStep(F\"12:   {step12_elapsed_time} seconds\")\n",
    "logStep(F\"13:   {step13_elapsed_time} seconds\")\n",
    "logStep(F\"14:   {step14_elapsed_time} seconds - parquet cached query\")\n",
    "logStep(F\"15:   {step15_elapsed_time} seconds\")\n",
    "logStep(F\"16:   {step16_elapsed_time} seconds\")\n",
    "logStep(F\"17:   {step17_elapsed_time} seconds\")\n",
    "logStep(F\"TT:   {step00_elapsed_time + step01_elapsed_time + step02_elapsed_time + step03_elapsed_time + step04_elapsed_time + step05_elapsed_time + step06_elapsed_time + step07_elapsed_time + step08_elapsed_time + step09_elapsed_time + step10_elapsed_time + step11_elapsed_time + step12_elapsed_time + step13_elapsed_time + step14_elapsed_time + step15_elapsed_time + step16_elapsed_time + step17_elapsed_time} seconds\")\n",
    "logStep(F'06 Cached Query Reduction  : {100-(step09_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(F'14 Parquet Query Reduction : {100-(step14_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(\"END OF PROGRAM-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
