{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 22 - BIG DATA ANALYSIS WITH SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 00 - Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "notebookRunGroups": {
     "groupValue": ""
    },
    "outputId": "78c91763-b7de-43b3-ae23-1fb13e566086",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:42:33.888428\u001b[33m STEP 00 - ENVIRONMENT PREPARATION\u001b[37m----------------------------------------------------\n",
      "Copyright        : Copyright (c) 2001-2023 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved.\n",
      "OS Platform      : darwin\n",
      "OS Name          : posix\n",
      "OS HOME          : /Users/eneas\n",
      "OS uName         : Darwin\n",
      "OS NodeName      : MBPJES14M2.local\n",
      "OS Release       : 23.0.0\n",
      "OS Release Ver   : Darwin Kernel Version 23.0.0: Tue Aug 22 02:11:55 PDT 2023; root:xnu-10002.1.11~16/RELEASE_ARM64_T6020\n",
      "OS Machine       : arm64\n",
      "Process ID       : 41799\n",
      "Parent Process   : 40235\n",
      "OS User          : root\n",
      "OS User ID       : 501\n",
      "OS Group ID      : 20\n",
      "OS Effective ID  : 501\n",
      "OS Effective GID : 20\n",
      "Current dir      : /Users/eneas/Documents/GitHub/RB_Module_22_Big_Data_Challenge\n",
      "Python version   : 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:33:12) [Clang 15.0.7 ]\n",
      "Version info     : sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\n",
      "Python API Ver   : 1013\n",
      "Executable       : /Users/eneas/anaconda3/envs/myDev/bin/python\n",
      "Hadoop home      : None\n",
      "Spark version    : 2.0.1\n",
      "Spark home(Find) : /Users/eneas/anaconda3/envs/myDev/lib/python3.11/site-packages/pyspark\n",
      "Spark Home(Env)  : /Users/eneas/anaconda3/envs/myDev/lib/python3.11/site-packages/pyspark\n",
      "Spark UI         : http://localhost:4040\n",
      "Spark submit     : /Users/eneas/anaconda3/envs/myDev/lib/python3.11/site-packages/ipykernel_launcher.py\n",
      "Java home        : /Users/eneas/anaconda3/envs/myDev\n",
      "\u001b[37m2023-09-06 15:42:33.893370\u001b[33m STEP 00 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:42:33.893877\u001b[33m STEP 00 - ELAPSED TIME: 0:00:00.005500 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 00 - Environment Setup\n",
    "#\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil          as shu\n",
    "import datetime\n",
    "import warnings   \n",
    "import findspark    \n",
    "from   colorama    import Fore                       \n",
    "from   pyspark     import SparkFiles  \n",
    "from   pyspark.sql import SparkSession\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "w, h       = shu.get_terminal_size()\n",
    "\n",
    "def logStep(msg):\n",
    "    l1 = len(msg)\n",
    "    l2 = w - l1\n",
    "    print(Fore.WHITE + str(datetime.datetime.now()) +  Fore.YELLOW + ' STEP ' + msg + Fore.WHITE + '-' * l2  )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "logStep('00 - ENVIRONMENT PREPARATION')\n",
    "warnings.filterwarnings('ignore')\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def spark_SQL_Run(step_Number, step_Message1, sql_Statement):\n",
    "  start_time         = datetime.datetime.now()\n",
    "  logStep(step_Number + ' - ' + step_Message1)\n",
    "  print()\n",
    "  try:\n",
    "    spark.sql(sql_Statement).show(5)\n",
    "  except Exception as e:\n",
    "    logStep(F\"{step_Number} - Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "  else:\n",
    "    logStep(step_Number + ' - ' + 'Spark SQL Statement Executed Successfully')\n",
    "  logStep(step_Number + ' - ' + 'DONE')\n",
    "  end_time           = datetime.datetime.now()\n",
    "  step_elapsed_time  = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "def runtime_Diff(step_Number, step_Message1,base_SQL, cached_SQL):\n",
    "  start_time          = datetime.datetime.now()\n",
    "  logStep(step_Number + \" - RUNTIME DIFFERENCE\")\n",
    "  time_difference     = base_SQL - cached_SQL\n",
    "  logStep(F\"{step_Number} - Time required for a non-cached Query : {base_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time required for a cached/part Query: {cached_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time difference                      : {time_difference}\")\n",
    "  logStep(step_Number + \" - DONE\")\n",
    "  end_time            = datetime.datetime.now()\n",
    "  step_elapsed_time   = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "print(F'Copyright        : {sys.copyright}')\n",
    "print(F'OS Platform      : {sys.platform}')\n",
    "print(F'OS Name          : {os.name}')\n",
    "print(F'OS HOME          : {os.environ.get(\"HOME\")}')\n",
    "print(F'OS uName         : {os.uname().sysname}')\n",
    "print(F'OS NodeName      : {os.uname().nodename}')\n",
    "print(F'OS Release       : {os.uname().release}')\n",
    "print(F'OS Release Ver   : {os.uname().version}')\n",
    "print(F'OS Machine       : {os.uname().machine}')\n",
    "print(F'Process ID       : {os.getpid()}')\n",
    "print(F'Parent Process   : {os.getppid()}')\n",
    "print(F'OS User          : {os.getlogin()}')\n",
    "print(F'OS User ID       : {os.getuid()}')\n",
    "print(F'OS Group ID      : {os.getgid()}')\n",
    "print(F'OS Effective ID  : {os.geteuid()}')\n",
    "print(F'OS Effective GID : {os.getegid()}')\n",
    "print(F'Current dir      : {os.getcwd()}')\n",
    "print(F'Python version   : {sys.version}')\n",
    "print(F'Version info     : {sys.version_info}')\n",
    "print(F'Python API Ver   : {sys.api_version}')\n",
    "print(F'Executable       : {sys.executable}')\n",
    "print(F'Hadoop home      : {os.environ.get(\"HADOOP_HOME\")}')\n",
    "print(F'Spark version    : {findspark.__version__}')\n",
    "print(F'Spark home(Find) : {findspark.find()}')\n",
    "print(F'Spark Home(Env)  : {os.environ.get(\"SPARK_HOME\")}')\n",
    "print(F'Spark UI         : http://localhost:4040')\n",
    "print(F'Spark submit     : {sys.argv[0]}')\n",
    "print(F'Java home        : {os.environ.get(\"JAVA_HOME\")}')\n",
    "\n",
    "logStep(\"00 - DONE\");\n",
    "end_time            = datetime.datetime.now()\n",
    "step00_elapsed_time = end_time - start_time\n",
    "logStep(F\"00 - ELAPSED TIME: {step00_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01 - Read in the source file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "69591c83-18c1-4e2a-86d4-2e5219c6d006",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:50.018445\u001b[33m STEP 01 - READ SOURCE DATA\u001b[37m-----------------------------------------------------------\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:38:53.116901\u001b[33m STEP 01 - Spark operation - Source data read successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-06 15:38:53.117406\u001b[33m STEP 01 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:53.117808\u001b[33m STEP 01 - ELAPSED TIME: 0:00:03.099344 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 01 - Read in the source file into a DataFrame.\n",
    "#\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logStep(\"01 - READ SOURCE DATA\");\n",
    "\n",
    "try:\n",
    "\n",
    "  url   = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "  spark.sparkContext.addFile(url)\n",
    "  home_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True)\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"01 - Spark operation - Source data read successfully\")\n",
    "\n",
    "logStep(\"01 - DONE\");\n",
    "end_time           = datetime.datetime.now()\n",
    "step01_elapsed_time = end_time - start_time\n",
    "logStep(F\"01 - ELAPSED TIME: {step01_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02 - Create a temporary view of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RoljcJ7WPpnm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:53.124075\u001b[33m STEP 02 - CREATE A TEMPORARY VIEW\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:53.143983\u001b[33m STEP 02 - Spark operation - Temporary view created successfully\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:38:53.144741\u001b[33m STEP 02 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:53.145619\u001b[33m STEP 02 - ELAPSED TIME: 0:00:00.021517 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 02 - Create a temporary view of the DataFrame.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"02 - CREATE A TEMPORARY VIEW\")\n",
    "\n",
    "try:\n",
    "  home_df.createOrReplaceTempView('home_sales')\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"02 - Spark operation - Temporary view created successfully\")\n",
    "\n",
    "logStep(\"02 - DONE\")\n",
    "end_time           = datetime.datetime.now()\n",
    "step02_elapsed_time = end_time - start_time\n",
    "logStep(F\"02 - ELAPSED TIME: {step02_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "7d9b6b22-c722-42dc-80c0-81e8810f7521",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:53.150893\u001b[33m STEP 03 - WHAT IS THE AVERAGE PRICE?=4BR\u001b[37m---------------------------------------------\n",
      "\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2022|296363.88|\n",
      "|2021|301819.44|\n",
      "|2020|298353.78|\n",
      "|2019| 300263.7|\n",
      "+----+---------+\n",
      "\n",
      "\u001b[37m2023-09-06 15:38:53.829138\u001b[33m STEP 03 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 15:38:53.829771\u001b[33m STEP 03 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:53.830076\u001b[33m STEP 03 - ELAPSED TIME: 0:00:00.679180 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step03_elapsed_time = spark_SQL_Run(\"03\", \"WHAT IS THE AVERAGE PRICE?=4BR\", \"SELECT YEAR(date) AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 4 GROUP BY YEAR(date) ORDER BY YEAR(date) DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "e1c36565-5164-4a9f-f36a-b2ad94522f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:53.835477\u001b[33m STEP 04 - WHAT IS THE AVERAGE PRICE?=3BR/3B\u001b[37m------------------------------------------\n",
      "\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|292676.79|\n",
      "|2016|290555.07|\n",
      "|2015| 288770.3|\n",
      "|2014|290852.27|\n",
      "|2013|295962.27|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:38:54.096187\u001b[33m STEP 04 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.096834\u001b[33m STEP 04 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.097276\u001b[33m STEP 04 - ELAPSED TIME: 0:00:00.261797 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step04_elapsed_time = spark_SQL_Run(\"04\", \"WHAT IS THE AVERAGE PRICE?=3BR/3B\", \"SELECT date_built AS YEAR , ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "9b51b8bc-8d1e-4a4e-ff50-f94a621891be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:54.102800\u001b[33m STEP 05 - WHAT IS THE AVERAGE PRICE?=3R/3/2B\u001b[37m-----------------------------------------\n",
      "\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|280317.58|\n",
      "|2016| 293965.1|\n",
      "|2015|297609.97|\n",
      "|2014|298264.72|\n",
      "|2013|303676.79|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:38:54.302639\u001b[33m STEP 05 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.303114\u001b[33m STEP 05 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.303478\u001b[33m STEP 05 - ELAPSED TIME: 0:00:00.200675 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step05_elapsed_time = spark_SQL_Run(\"05\", \"WHAT IS THE AVERAGE PRICE?=3R/3/2B\", \"SELECT date_built AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 AND floors == 2 AND sqft_living >= 2000 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "73e46678-fca9-434f-feb8-b2460e96e41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:54.308616\u001b[33m STEP 06 - WHAT IS THE AVERAGE PRICE?=300K\u001b[37m--------------------------------------------\n",
      "\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:38:54.530306\u001b[33m STEP 06 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.531601\u001b[33m STEP 06 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.532921\u001b[33m STEP 06 - ELAPSED TIME: 0:00:00.224302 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Ste 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "#\n",
    "\n",
    "step06_elapsed_time = spark_SQL_Run(\"06\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 07 - Cache the the temporary table home_sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "7a421ad4-699e-4ae1-c13e-3ac1379a3cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:54.539895\u001b[33m STEP 07 - CACHE HOME DATA\u001b[37m------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.894268\u001b[33m STEP 07 - Spark operation - Cache executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.895134\u001b[33m STEP 07 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.895586\u001b[33m STEP 07 - ELAPSED TIME: 0:00:00.355233 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 07 - Cache the the temporary table home_sales.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"07 - CACHE HOME DATA\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"cache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1) \n",
    "else:\n",
    "  logStep(\"07 - Spark operation - Cache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"07 - DONE\")\n",
    "step07_elapsed_time = end_time - start_time\n",
    "logStep(F\"07 - ELAPSED TIME: {step07_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 08 - Check if the table is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "3003ef79-0678-4a80-e79a-eac9fdfc8a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:54.900766\u001b[33m STEP 08 - IS THE DATA CACHED?\u001b[37m--------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.907527\u001b[33m STEP 08 - home_sales is cached\u001b[37m-------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.908253\u001b[33m STEP 08 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.908649\u001b[33m STEP 08 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:54.909015\u001b[33m STEP 08 - ELAPSED TIME: 0:00:00.007880 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 08 - Check if the table is cached.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"08 - IS THE DATA CACHED?\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "    logStep(\"08 - home_sales is not cached\")\n",
    "  else:\n",
    "    logStep(\"08 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "    logStep(F\"Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logStep(\"08 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"08 - DONE\")\n",
    "step08_elapsed_time = end_time - start_time\n",
    "logStep(F\"08 - ELAPSED TIME: {step08_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "a7aec34a-063b-46bc-b428-8660a32d9a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:38:54.913220\u001b[33m STEP 09 - WHAT IS THE AVERAGE PRICE?=300K\u001b[37m--------------------------------------------\n",
      "\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:38:55.021231\u001b[33m STEP 09 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 15:38:55.021878\u001b[33m STEP 09 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:38:55.022402\u001b[33m STEP 09 - ELAPSED TIME: 0:00:00.109168 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "#\n",
    "\n",
    "step09_elapsed_time = spark_SQL_Run(\"09\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAYsuFaZuCmw",
    "outputId": "c5787884-c516-4f61-9956-6edfc0dcbf9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:44:29.209539\u001b[33m STEP 10 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:29.210254\u001b[33m STEP 10 - Time required for a non-cached Query : 0:00:00.224302\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:44:29.210501\u001b[33m STEP 10 - Time required for a cached/part Query: 0:00:00.109168\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:44:29.210896\u001b[33m STEP 10 - Time difference                      : 0:00:00.115134\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:44:29.211361\u001b[33m STEP 10 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:29.211735\u001b[33m STEP 10 - ELAPSED TIME: 0:00:00.002195 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 10 Determine the runtime and compare to the original runtime\n",
    "#\n",
    "\n",
    "step10_elapsed_time = runtime_Diff(\"10\", \"RUNTIME DIFFERENCE\", step06_elapsed_time, step09_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:42:46.213155\u001b[33m STEP 11 - FORMATTED PARQUET\u001b[37m----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:42:47.395289\u001b[33m STEP 11 - Spark operation - Parquet file created successfully.\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-06 15:42:47.395961\u001b[33m STEP 11 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:42:47.396365\u001b[33m STEP 11 - ELAPSED TIME: 0:00:01.182798 seconds\u001b[37m---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"11 - FORMATTED PARQUET\")\n",
    "\n",
    "try:\n",
    "  home_df.write.parquet('home_parquet', mode='overwrite',partitionBy='date_built')\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"11 - Spark operation - Parquet file created successfully.\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"11 - DONE\")\n",
    "step11_elapsed_time = end_time - start_time\n",
    "logStep(F\"11 - ELAPSED TIME: {step11_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12 - Read the parquet formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:42:49.309220\u001b[33m STEP 12 - READ PARQUET\u001b[37m---------------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:42:49.518165\u001b[33m STEP 12 - Spark operation - Parquet file read successfully\u001b[37m---------------------------\n",
      "\u001b[37m2023-09-06 15:42:49.518694\u001b[33m STEP 12 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:42:49.519051\u001b[33m STEP 12 - ELAPSED TIME: 0:00:00.209469 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 12 - Read the parquet formatted data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"12 - READ PARQUET\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df = spark.read.parquet('home_parquet')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"12 - Spark operation - Parquet file read successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"12 - DONE\")\n",
    "step12_elapsed_time = end_time - start_time\n",
    "logStep(F\"12 - ELAPSED TIME: {step12_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13 - Create a temporary table for the parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:42:52.187642\u001b[33m STEP 13 - CREATE PARQUET VIEW\u001b[37m--------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:42:52.232128\u001b[33m STEP 13 - Spark operation - Parquet view created successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 15:42:52.232766\u001b[33m STEP 13 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:42:52.233167\u001b[33m STEP 13 - ELAPSED TIME: 0:00:00.045117 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 13 - Create a temporary table for the parquet data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"13 - CREATE PARQUET VIEW\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df.createOrReplaceTempView('parquet_temp_home')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  print(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"13 - Spark operation - Parquet view created successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"13 - DONE\")\n",
    "step13_elapsed_time = end_time - start_time\n",
    "logStep(F\"13 - ELAPSED TIME: {step13_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "5420b7c8-7f4b-404f-fc91-f70a06039866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:42:54.637088\u001b[33m STEP 14 - 14 - REPEAT QUERY\u001b[37m----------------------------------------------------------\n",
      "\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 15:42:54.835251\u001b[33m STEP 14 - Spark SQL Statement Executed Successfully\u001b[37m----------------------------------\n",
      "\u001b[37m2023-09-06 15:42:54.836230\u001b[33m STEP 14 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:42:54.836671\u001b[33m STEP 14 - ELAPSED TIME: 0:00:00.199581 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places. \n",
    "#\n",
    "\n",
    "step14_elapsed_time = spark_SQL_Run(\"14\", \"14 - REPEAT QUERY\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM parquet_temp_home GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 15 Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEkIu5yYuWlW",
    "outputId": "db861ec0-49c1-4855-86b4-f360d566302e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:44:39.308880\u001b[33m STEP 15 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:39.310958\u001b[33m STEP 15 - Time required for a non-cached Query : 0:00:00.224302\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:44:39.313370\u001b[33m STEP 15 - Time required for a cached/part Query: 0:00:00.199581\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:44:39.315575\u001b[33m STEP 15 - Time difference                      : 0:00:00.024721\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 15:44:39.317432\u001b[33m STEP 15 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:39.319281\u001b[33m STEP 15 - ELAPSED TIME: 0:00:00.010391 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Step 15 - Determine the runtime and compare it to the cached version.\n",
    "#\n",
    "\n",
    "step15_elapsed_time = runtime_Diff(\"15\", \"RUNTIME DIFFERENCE\", step06_elapsed_time, step14_elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 16 - Un-cache the home_sales temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "5b8e7eaf-78eb-4eef-d29d-1be425668fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:43:02.181146\u001b[33m STEP 16 - UNCACHE\u001b[37m--------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:43:02.194400\u001b[33m STEP 16 - Spark operation - Uncache executed successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-06 15:43:02.194980\u001b[33m STEP 16 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:43:02.195353\u001b[33m STEP 16 - ELAPSED TIME: 0:00:00.013829 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 16 - Un-cache the home_sales temporary table.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"16 - UNCACHE\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"uncache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"16 - Spark operation - Uncache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"16 - DONE\")\n",
    "step16_elapsed_time = end_time - start_time\n",
    "logStep(F\"16 - ELAPSED TIME: {step16_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 17 - Check if the home_sales is no longer cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "a3d33ffb-4052-41c6-8040-bff5a22a2188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:43:10.838065\u001b[33m STEP 17 - CACHE CHECK\u001b[37m----------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:43:10.841869\u001b[33m STEP 17 - home_sales is not cached\u001b[37m---------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:43:10.842402\u001b[33m STEP 17 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 15:43:10.842864\u001b[33m STEP 17 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:43:10.843466\u001b[33m STEP 17 - ELAPSED TIME: 0:00:00.004799 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Step 17 - Check if the home_sales is no longer cached\n",
    "#\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"17 - CACHE CHECK\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "      logStep(\"17 - home_sales is not cached\")\n",
    "  else:\n",
    "      logStep(\"17 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"17 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"17 - DONE\")\n",
    "step17_elapsed_time = end_time - start_time\n",
    "logStep(F\"17 - ELAPSED TIME: {step17_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 15:44:45.355053\u001b[33m STEP TOTAL RUN TIME\u001b[37m------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.355792\u001b[33m STEP  0:   0:00:00.005500 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.356253\u001b[33m STEP  1:   0:00:03.099344 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.356555\u001b[33m STEP  2:   0:00:00.021517 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.357011\u001b[33m STEP  3:   0:00:00.679180 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.357485\u001b[33m STEP  4:   0:00:00.261797 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.357705\u001b[33m STEP  5:   0:00:00.200675 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.358107\u001b[33m STEP  6:   0:00:00.224302 seconds - non-cached query\u001b[37m---------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.358329\u001b[33m STEP  7:   0:00:00.355233 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.358679\u001b[33m STEP  8:   0:00:00.007880 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.359004\u001b[33m STEP  9:   0:00:00.109168 seconds - cached query\u001b[37m-------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.359248\u001b[33m STEP 10:   0:00:00.002195 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.359477\u001b[33m STEP 11:   0:00:01.182798 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.359769\u001b[33m STEP 12:   0:00:00.209469 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.360211\u001b[33m STEP 13:   0:00:00.045117 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.360448\u001b[33m STEP 14:   0:00:00.199581 seconds - parquet cached query\u001b[37m-----------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.360672\u001b[33m STEP 15:   0:00:00.010391 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.361126\u001b[33m STEP 16:   0:00:00.013829 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.361365\u001b[33m STEP 17:   0:00:00.004799 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.361664\u001b[33m STEP TT:   0:00:06.632775 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.361912\u001b[33m STEP 06 Cached Query Reduction  : 51.33%\u001b[37m---------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.362141\u001b[33m STEP 14 Parquet Query Reduction : 11.02%\u001b[37m---------------------------------------------\n",
      "\u001b[37m2023-09-06 15:44:45.362368\u001b[33m STEP END OF PROGRAM-\u001b[37m-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "logStep(\"TOTAL RUN TIME\")\n",
    "logStep(F\" 0:   {step00_elapsed_time} seconds\")\n",
    "logStep(F\" 1:   {step01_elapsed_time} seconds\")\n",
    "logStep(F\" 2:   {step02_elapsed_time} seconds\")\n",
    "logStep(F\" 3:   {step03_elapsed_time} seconds\")\n",
    "logStep(F\" 4:   {step04_elapsed_time} seconds\")\n",
    "logStep(F\" 5:   {step05_elapsed_time} seconds\")\n",
    "logStep(F\" 6:   {step06_elapsed_time} seconds - non-cached query\")\n",
    "logStep(F\" 7:   {step07_elapsed_time} seconds\")\n",
    "logStep(F\" 8:   {step08_elapsed_time} seconds\")\n",
    "logStep(F\" 9:   {step09_elapsed_time} seconds - cached query\")\n",
    "logStep(F\"10:   {step10_elapsed_time} seconds\")\n",
    "logStep(F\"11:   {step11_elapsed_time} seconds\")\n",
    "logStep(F\"12:   {step12_elapsed_time} seconds\")\n",
    "logStep(F\"13:   {step13_elapsed_time} seconds\")\n",
    "logStep(F\"14:   {step14_elapsed_time} seconds - parquet cached query\")\n",
    "logStep(F\"15:   {step15_elapsed_time} seconds\")\n",
    "logStep(F\"16:   {step16_elapsed_time} seconds\")\n",
    "logStep(F\"17:   {step17_elapsed_time} seconds\")\n",
    "logStep(F\"TT:   {step00_elapsed_time + step01_elapsed_time + step02_elapsed_time + step03_elapsed_time + step04_elapsed_time + step05_elapsed_time + step06_elapsed_time + step07_elapsed_time + step08_elapsed_time + step09_elapsed_time + step10_elapsed_time + step11_elapsed_time + step12_elapsed_time + step13_elapsed_time + step14_elapsed_time + step15_elapsed_time + step16_elapsed_time + step17_elapsed_time} seconds\")\n",
    "logStep(F'06 Cached Query Reduction  : {100-(step09_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(F'14 Parquet Query Reduction : {100-(step14_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(\"END OF PROGRAM-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
