{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULE 22 - BIG DATA ANALYSIS WITH SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 00 - Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "notebookRunGroups": {
     "groupValue": ""
    },
    "outputId": "78c91763-b7de-43b3-ae23-1fb13e566086",
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:37.857608\u001b[33m STEP 00 - ENVIRONMENT PREPARATION\u001b[37m----------------------------------------------------\n",
      "Copyright        : Copyright (c) 2001-2022 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved.\n",
      "OS Platform      : darwin\n",
      "OS Name          : posix\n",
      "OS HOME          : /Users/eneas\n",
      "OS uName         : Darwin\n",
      "OS NodeName      : MBPJES14M2.local\n",
      "OS Release       : 23.0.0\n",
      "OS Release Ver   : Darwin Kernel Version 23.0.0: Tue Aug 22 02:11:55 PDT 2023; root:xnu-10002.1.11~16/RELEASE_ARM64_T6020\n",
      "OS Machine       : arm64\n",
      "Process ID       : 22319\n",
      "Parent Process   : 22154\n",
      "OS User          : root\n",
      "OS User ID       : 501\n",
      "OS Group ID      : 20\n",
      "OS Effective ID  : 501\n",
      "OS Effective GID : 20\n",
      "Current dir      : /Users/eneas/Documents/GitHub/RB_Module_22_Big_Data_Challenge\n",
      "Python version   : 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]\n",
      "Version info     : sys.version_info(major=3, minor=10, micro=8, releaselevel='final', serial=0)\n",
      "Python API Ver   : 1013\n",
      "Executable       : /Users/eneas/anaconda3/bin/python\n",
      "Hadoop home      : None\n",
      "Spark version    : 2.0.1\n",
      "Spark home(Find) : /Users/eneas/anaconda3/lib/python3.10/site-packages/pyspark\n",
      "Spark Home(Env)  : None\n",
      "Spark UI         : http://localhost:4040\n",
      "Spark submit     : /Users/eneas/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\n",
      "Java home        : /Users/eneas/anaconda3\n",
      "\u001b[37m2023-09-06 10:41:37.859134\u001b[33m STEP 00 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:37.859459\u001b[33m STEP 00 - ELAPSED TIME: 0:00:00.001948 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "#\n",
    "# Step 00 - Environment Setup\n",
    "#\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil          as shu\n",
    "import datetime\n",
    "import warnings   \n",
    "import findspark    \n",
    "from   colorama    import Fore                       \n",
    "from   pyspark     import SparkFiles  \n",
    "from   pyspark.sql import SparkSession\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "w, h       = shu.get_terminal_size()\n",
    "\n",
    "def logStep(msg):\n",
    "    l1 = len(msg)\n",
    "    l2 = w - l1\n",
    "    print(Fore.WHITE + str(datetime.datetime.now()) +  Fore.YELLOW + ' STEP ' + msg + Fore.WHITE + '-' * l2  )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "logStep('00 - ENVIRONMENT PREPARATION')\n",
    "warnings.filterwarnings('ignore')\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def spark_SQL_Run(step_Number, step_Message1, sql_Statement):\n",
    "  start_time         = datetime.datetime.now()\n",
    "  logStep(step_Number + ' - ' + step_Message1)\n",
    "  print()\n",
    "  try:\n",
    "    spark.sql(sql_Statement).show(5)\n",
    "  except Exception as e:\n",
    "    logStep(F\"{step_Number} - Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "  else:\n",
    "    logStep(step_Number + ' - ' + 'Spark SQL Statement Executed Successfully')\n",
    "  logStep(step_Number + ' - ' + 'DONE')\n",
    "  end_time           = datetime.datetime.now()\n",
    "  step_elapsed_time  = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "print(F'Copyright        : {sys.copyright}')\n",
    "print(F'OS Platform      : {sys.platform}')\n",
    "print(F'OS Name          : {os.name}')\n",
    "print(F'OS HOME          : {os.environ.get(\"HOME\")}')\n",
    "print(F'OS uName         : {os.uname().sysname}')\n",
    "print(F'OS NodeName      : {os.uname().nodename}')\n",
    "print(F'OS Release       : {os.uname().release}')\n",
    "print(F'OS Release Ver   : {os.uname().version}')\n",
    "print(F'OS Machine       : {os.uname().machine}')\n",
    "print(F'Process ID       : {os.getpid()}')\n",
    "print(F'Parent Process   : {os.getppid()}')\n",
    "print(F'OS User          : {os.getlogin()}')\n",
    "print(F'OS User ID       : {os.getuid()}')\n",
    "print(F'OS Group ID      : {os.getgid()}')\n",
    "print(F'OS Effective ID  : {os.geteuid()}')\n",
    "print(F'OS Effective GID : {os.getegid()}')\n",
    "print(F'Current dir      : {os.getcwd()}')\n",
    "print(F'Python version   : {sys.version}')\n",
    "print(F'Version info     : {sys.version_info}')\n",
    "print(F'Python API Ver   : {sys.api_version}')\n",
    "print(F'Executable       : {sys.executable}')\n",
    "print(F'Hadoop home      : {os.environ.get(\"HADOOP_HOME\")}')\n",
    "print(F'Spark version    : {findspark.__version__}')\n",
    "print(F'Spark home(Find) : {findspark.find()}')\n",
    "print(F'Spark Home(Env)  : {os.environ.get(\"SPARK_HOME\")}')\n",
    "print(F'Spark UI         : http://localhost:4040')\n",
    "print(F'Spark submit     : {sys.argv[0]}')\n",
    "print(F'Java home        : {os.environ.get(\"JAVA_HOME\")}')\n",
    "\n",
    "logStep(\"00 - DONE\");\n",
    "end_time            = datetime.datetime.now()\n",
    "step00_elapsed_time = end_time - start_time\n",
    "logStep(F\"00 - ELAPSED TIME: {step00_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01 - Read in the source file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "69591c83-18c1-4e2a-86d4-2e5219c6d006",
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:37.865447\u001b[33m STEP 01 - READ SOURCE DATA\u001b[37m-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 10:41:38 WARN Utils: Your hostname, MBPJES14M2.local resolves to a loopback address: 127.0.0.1; using 10.0.0.237 instead (on interface en0)\n",
      "23/09/06 10:41:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/06 10:41:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/06 10:41:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:43.483529\u001b[33m STEP 01 - Spark operation - Source data read successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-06 10:41:43.484172\u001b[33m STEP 01 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:43.484650\u001b[33m STEP 01 - ELAPSED TIME: 0:00:05.619170 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "#\n",
    "# Step 01 - Read in the source file into a DataFrame.\n",
    "#\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logStep(\"01 - READ SOURCE DATA\");\n",
    "\n",
    "try:\n",
    "\n",
    "  url   = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "  spark.sparkContext.addFile(url)\n",
    "  home_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True)\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"01 - Spark operation - Source data read successfully\")\n",
    "\n",
    "logStep(\"01 - DONE\");\n",
    "end_time           = datetime.datetime.now()\n",
    "step01_elapsed_time = end_time - start_time\n",
    "logStep(F\"01 - ELAPSED TIME: {step01_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02 - Create a temporary view of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoljcJ7WPpnm",
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:43.489731\u001b[33m STEP 02 - CREATE A TEMPORARY VIEW\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:43.512665\u001b[33m STEP 02 - Spark operation - Temporary view created successfully\u001b[37m----------------------\n",
      "\u001b[37m2023-09-06 10:41:43.513511\u001b[33m STEP 02 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:43.514123\u001b[33m STEP 02 - ELAPSED TIME: 0:00:00.024356 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "#\n",
    "# Step 02 - Create a temporary view of the DataFrame.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"02 - CREATE A TEMPORARY VIEW\")\n",
    "\n",
    "try:\n",
    "  home_df.createOrReplaceTempView('home_sales')\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"02 - Spark operation - Temporary view created successfully\")\n",
    "\n",
    "logStep(\"02 - DONE\")\n",
    "end_time           = datetime.datetime.now()\n",
    "step02_elapsed_time = end_time - start_time\n",
    "logStep(F\"02 - ELAPSED TIME: {step02_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "7d9b6b22-c722-42dc-80c0-81e8810f7521",
    "tags": []
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:43.521313\u001b[33m STEP 03 - WHAT IS THE AVERAGE PRICE?=4BR\u001b[37m---------------------------------------------\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2022|296363.88|\n",
      "|2021|301819.44|\n",
      "|2020|298353.78|\n",
      "|2019| 300263.7|\n",
      "+----+---------+\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:44.316436\u001b[33m STEP 03 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 10:41:44.316999\u001b[33m STEP 03 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:44.317980\u001b[33m STEP 03 - ELAPSED TIME: 0:00:00.796638 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 03 - What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step03_elapsed_time = spark_SQL_Run(\"03\", \"WHAT IS THE AVERAGE PRICE?=4BR\", \"SELECT YEAR(date) AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 4 GROUP BY YEAR(date) ORDER BY YEAR(date) DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "e1c36565-5164-4a9f-f36a-b2ad94522f5e"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:44.322899\u001b[33m STEP 04 - WHAT IS THE AVERAGE PRICE?=3BR/3B\u001b[37m------------------------------------------\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|292676.79|\n",
      "|2016|290555.07|\n",
      "|2015| 288770.3|\n",
      "|2014|290852.27|\n",
      "|2013|295962.27|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:44.621303\u001b[33m STEP 04 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 10:41:44.621935\u001b[33m STEP 04 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:44.622521\u001b[33m STEP 04 - ELAPSED TIME: 0:00:00.299033 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 04 - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step04_elapsed_time = spark_SQL_Run(\"04\", \"WHAT IS THE AVERAGE PRICE?=3BR/3B\", \"SELECT date_built AS YEAR , ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "9b51b8bc-8d1e-4a4e-ff50-f94a621891be"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:44.627826\u001b[33m STEP 05 - WHAT IS THE AVERAGE PRICE?=3R/3/2\u001b[37m------------------------------------------\n",
      "+----+---------+\n",
      "|YEAR|  AVERAGE|\n",
      "+----+---------+\n",
      "|2017|280317.58|\n",
      "|2016| 293965.1|\n",
      "|2015|297609.97|\n",
      "|2014|298264.72|\n",
      "|2013|303676.79|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:44.852281\u001b[33m STEP 05 = Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 10:41:44.852978\u001b[33m STEP 05 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:44.853472\u001b[33m STEP 05 - ELAPSED TIME: 0:00:00.225143 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "#\n",
    "# Step 05 - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
    "#\n",
    "\n",
    "step05_elapsed_time = spark_SQL_Run(\"05\", \"WHAT IS THE AVERAGE PRICE?=3R/3/2B\", \"SELECT date_built AS YEAR, ROUND(AVG(price),2) AS AVERAGE FROM home_sales WHERE bedrooms == 3 AND bathrooms == 3 AND floors == 2 AND sqft_living >= 2000 GROUP BY date_built ORDER BY date_built DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "73e46678-fca9-434f-feb8-b2460e96e41c"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:44.861236\u001b[33m STEP 06 - WHAT IS THE AVERAGE PRICE?=300K\u001b[37m--------------------------------------------\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:45.117861\u001b[33m STEP 06 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.118558\u001b[33m STEP 06 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.119030\u001b[33m STEP 06 - ELAPSED TIME: 0:00:00.257320 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Ste 06 - What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
    "#\n",
    "\n",
    "step06_elapsed_time = spark_SQL_Run(\"06\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 07 - Cache the the temporary table home_sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "7a421ad4-699e-4ae1-c13e-3ac1379a3cd6"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:45.123528\u001b[33m STEP 07 - CACHE HOME DATA\u001b[37m------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.575736\u001b[33m STEP 07 - Spark operation - Cache executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.576437\u001b[33m STEP 07 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.576984\u001b[33m STEP 07 - ELAPSED TIME: 0:00:00.452903 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 07 - Cache the the temporary table home_sales.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"07 - CACHE HOME DATA\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"cache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1) \n",
    "else:\n",
    "  logStep(\"07 - Spark operation - Cache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"07 - DONE\")\n",
    "step07_elapsed_time = end_time - start_time\n",
    "logStep(F\"07 - ELAPSED TIME: {step07_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 08 - Check if the table is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "3003ef79-0678-4a80-e79a-eac9fdfc8a6f"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:45.581770\u001b[33m STEP 08 - IS THE DATA CACHED?\u001b[37m--------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.591970\u001b[33m STEP 08 - home_sales is cached\u001b[37m-------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.592541\u001b[33m STEP 08 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.593218\u001b[33m STEP 08 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.593732\u001b[33m STEP 08 - ELAPSED TIME: 0:00:00.011409 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 08 - Check if the table is cached.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"08 - IS THE DATA CACHED?\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "    logStep(\"08 - home_sales is not cached\")\n",
    "  else:\n",
    "    logStep(\"08 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "    logStep(F\"Exception: {e}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logStep(\"08 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"08 - DONE\")\n",
    "step08_elapsed_time = end_time - start_time\n",
    "logStep(F\"08 - ELAPSED TIME: {step08_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "a7aec34a-063b-46bc-b428-8660a32d9a58"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:45.599750\u001b[33m STEP 09 - REPEAT QUERY\u001b[37m---------------------------------------------------------------\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:45.736200\u001b[33m STEP 09 - Spark operation - Query executed successfully\u001b[37m------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.736860\u001b[33m STEP 09 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.737251\u001b[33m STEP 09 - ELAPSED TIME: 0:00:00.137105 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 09 - Using the cached data, run the query that filters out the view ratings with average price greater than or equal to $350,000. Determine the runtime and compare it to uncached runtime.\n",
    "#\n",
    "\n",
    "step09_elapsed_time = spark_SQL_Run(\"09\", \"WHAT IS THE AVERAGE PRICE?=300K\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM home_sales GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10 - Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAYsuFaZuCmw",
    "outputId": "c5787884-c516-4f61-9956-6edfc0dcbf9a"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:45.741917\u001b[33m STEP 10 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.742977\u001b[33m STEP 10 - Time required for a non-cached Query: 0:00:00.257320\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-06 10:41:45.743526\u001b[33m STEP 10 - Time required for a cached Query    : 0:00:00.137105\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-06 10:41:45.744322\u001b[33m STEP 10 - Time difference                     : 0:00:00.120215\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-06 10:41:45.745420\u001b[33m STEP 10 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:45.745897\u001b[33m STEP 10 - ELAPSED TIME: 0:00:00.003501 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 10 Determine the runtime and compare to the original runtime\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"10 - RUNTIME DIFFERENCE\")\n",
    "\n",
    "time_difference = step06_elapsed_time - step09_elapsed_time\n",
    "logStep(F\"10 - Time required for a non-cached Query: {step06_elapsed_time}\")\n",
    "logStep(F\"10 - Time required for a cached Query    : {step09_elapsed_time}\")\n",
    "logStep(F\"10 - Time difference                     : {time_difference}\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"10 - DONE\")\n",
    "step10_elapsed_time = end_time - start_time\n",
    "logStep(F\"10 - ELAPSED TIME: {step10_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:45.752324\u001b[33m STEP 11 - FORMATTED PARQUET\u001b[37m----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:46.845170\u001b[33m STEP 11 - Spark operation - Parquet file created successfully.\u001b[37m-----------------------\n",
      "\u001b[37m2023-09-06 10:41:46.845709\u001b[33m STEP 11 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:46.846162\u001b[33m STEP 11 - ELAPSED TIME: 0:00:01.093380 seconds\u001b[37m---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 11 - Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"11 - FORMATTED PARQUET\")\n",
    "\n",
    "try:\n",
    "  home_df.write.parquet('home_parquet', mode='overwrite',partitionBy='date_built')\n",
    "  home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"11 - Spark operation - Parquet file created successfully.\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"11 - DONE\")\n",
    "step11_elapsed_time = end_time - start_time\n",
    "logStep(F\"11 - ELAPSED TIME: {step11_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12 - Read the parquet formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:46.850461\u001b[33m STEP 12 - READ PARQUET\u001b[37m---------------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:47.047938\u001b[33m STEP 12 - Spark operation - Parquet file read successfully\u001b[37m---------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.048510\u001b[33m STEP 12 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.048961\u001b[33m STEP 12 - ELAPSED TIME: 0:00:00.198044 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 12 - Read the parquet formatted data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"12 - READ PARQUET\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df = spark.read.parquet('home_parquet')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"12 - Spark operation - Parquet file read successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"12 - DONE\")\n",
    "step12_elapsed_time = end_time - start_time\n",
    "logStep(F\"12 - ELAPSED TIME: {step12_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13 - Create a temporary table for the parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:47.053404\u001b[33m STEP 13 - CREATE PARQUET VIEW\u001b[37m--------------------------------------------------------\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|date_built|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710|       3|        3|       1918|    9666|     1|         0|  25|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896|       3|        3|       2197|    8641|     1|         0|   3|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650|       2|        3|       2100|   10419|     2|         0|   7|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313|       3|        3|       1960|    9453|     2|         0|   2|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574|       3|        2|       1635|    8040|     2|         0|  10|      2015|\n",
      "+--------------------+----------+------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:47.109566\u001b[33m STEP 13 - Spark operation - Parquet view created successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.110314\u001b[33m STEP 13 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.110789\u001b[33m STEP 13 - ELAPSED TIME: 0:00:00.056898 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 13 - Create a temporary table for the parquet data.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"13 - CREATE PARQUET VIEW\")\n",
    "\n",
    "try:\n",
    "  parquet_home_df.createOrReplaceTempView('parquet_temp_home')\n",
    "  parquet_home_df.show(5)\n",
    "except Exception as e:\n",
    "  print(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"13 - Spark operation - Parquet view created successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"13 - DONE\")\n",
    "step13_elapsed_time = end_time - start_time\n",
    "logStep(F\"13 - ELAPSED TIME: {step13_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "5420b7c8-7f4b-404f-fc91-f70a06039866"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:47.115730\u001b[33m STEP 14 - REPEAT QUERY\u001b[37m---------------------------------------------------------------\n",
      "+----+----------+\n",
      "|VIEW|   AVERAGE|\n",
      "+----+----------+\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[37m2023-09-06 10:41:47.355487\u001b[33m STEP 14 - Spark operation - Parquet query executed successfully.\u001b[37m---------------------\n",
      "\u001b[37m2023-09-06 10:41:47.356097\u001b[33m STEP 14 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.356688\u001b[33m STEP 14 - ELAPSED TIME: 0:00:00.240361 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 14 - Run the query that filters out the view ratings with average price of greater than or equal to $350,000 with the parquet DataFrame. Round your average to two decimal places. \n",
    "#\n",
    "\n",
    "step14_elapsed_time = spark_SQL_Run(\"14\", \"14 - REPEAT QUERY\", \"SELECT view AS VIEW, ROUND(AVG(price),2) AS AVERAGE FROM parquet_temp_home GROUP BY view HAVING ROUND(AVG(price),2) >= 350000 ORDER BY view DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 15 Determine the runtime and compare to the original runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEkIu5yYuWlW",
    "outputId": "db861ec0-49c1-4855-86b4-f360d566302e"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:47.360953\u001b[33m STEP 15 - RUNTIME DIFFERENCE\u001b[37m---------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.361806\u001b[33m STEP 15 - Time required for a non-cached Query    : 0:00:00.257320\u001b[37m-------------------\n",
      "\u001b[37m2023-09-06 10:41:47.362236\u001b[33m STEP 15 - Time required for a parquet cached Query: 0:00:00.240361\u001b[37m-------------------\n",
      "\u001b[37m2023-09-06 10:41:47.362844\u001b[33m STEP 15 - Time difference                         : 0:00:00.016959\u001b[37m-------------------\n",
      "\u001b[37m2023-09-06 10:41:47.363128\u001b[33m STEP 15 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.363475\u001b[33m STEP 15 - ELAPSED TIME: 0:00:00.002502 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "#\n",
    "# Step 15 - Determine the runtime and compare it to the cached version.\n",
    "#\n",
    "\n",
    "start_time          = datetime.datetime.now()\n",
    "logStep(\"15 - RUNTIME DIFFERENCE\")\n",
    "\n",
    "time_difference     = step06_elapsed_time - step14_elapsed_time\n",
    "logStep(F\"15 - Time required for a non-cached Query    : {step06_elapsed_time}\")\n",
    "logStep(F\"15 - Time required for a parquet cached Query: {step14_elapsed_time}\")\n",
    "logStep(F\"15 - Time difference                         : {time_difference}\")\n",
    "logStep(\"15 - DONE\")\n",
    "end_time            = datetime.datetime.now()\n",
    "step15_elapsed_time = end_time - start_time\n",
    "logStep(F\"15 - ELAPSED TIME: {step15_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 16 - Un-cache the home_sales temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "5b8e7eaf-78eb-4eef-d29d-1be425668fd1"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:47.367368\u001b[33m STEP 16 - UNCACHE\u001b[37m--------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.387132\u001b[33m STEP 16 - Spark operation - Uncache executed successfully\u001b[37m----------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.387980\u001b[33m STEP 16 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.388435\u001b[33m STEP 16 - ELAPSED TIME: 0:00:00.020593 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 16 - Un-cache the home_sales temporary table.\n",
    "#\n",
    "\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"16 - UNCACHE\")\n",
    "\n",
    "try:\n",
    "  spark.sql(\"uncache table home_sales\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"16 - Spark operation - Uncache executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"16 - DONE\")\n",
    "step16_elapsed_time = end_time - start_time\n",
    "logStep(F\"16 - ELAPSED TIME: {step16_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 17 - Check if the home_sales is no longer cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "a3d33ffb-4052-41c6-8040-bff5a22a2188"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:47.393979\u001b[33m STEP 17 - CACHE CHECK\u001b[37m----------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.399279\u001b[33m STEP 17 - home_sales is not cached\u001b[37m---------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.399751\u001b[33m STEP 17 - Spark operation - Cache check executed successfully\u001b[37m------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.400190\u001b[33m STEP 17 - DONE\u001b[37m-----------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.400595\u001b[33m STEP 17 - ELAPSED TIME: 0:00:00.006206 seconds\u001b[37m---------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "# \n",
    "# Step 17 - Check if the home_sales is no longer cached\n",
    "#\n",
    "start_time         = datetime.datetime.now()\n",
    "logStep(\"17 - CACHE CHECK\")\n",
    "\n",
    "try:\n",
    "  if (spark.catalog.isCached('home_sales') == False):\n",
    "      logStep(\"17 - home_sales is not cached\")\n",
    "  else:\n",
    "      logStep(\"17 - home_sales is cached\")\n",
    "except Exception as e:\n",
    "  logStep(F\"Exception: {e}\")\n",
    "  sys.exit(1)\n",
    "else:\n",
    "  logStep(\"17 - Spark operation - Cache check executed successfully\")\n",
    "\n",
    "end_time           = datetime.datetime.now()\n",
    "logStep(\"17 - DONE\")\n",
    "step17_elapsed_time = end_time - start_time\n",
    "logStep(F\"17 - ELAPSED TIME: {step17_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
=======
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m2023-09-06 10:41:47.405863\u001b[33m STEP TOTAL RUN TIME\u001b[37m------------------------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.407362\u001b[33m STEP  0:   0:00:00.001948 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.407827\u001b[33m STEP  1:   0:00:05.619170 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.408243\u001b[33m STEP  2:   0:00:00.024356 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.408669\u001b[33m STEP  3:   0:00:00.796638 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.409069\u001b[33m STEP  4:   0:00:00.299033 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.409834\u001b[33m STEP  5:   0:00:00.225143 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.410257\u001b[33m STEP  6:   0:00:00.257320 seconds - non-cached query\u001b[37m---------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.410550\u001b[33m STEP  7:   0:00:00.452903 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.410928\u001b[33m STEP  8:   0:00:00.011409 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.411256\u001b[33m STEP  9:   0:00:00.137105 seconds - cached query\u001b[37m-------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.411597\u001b[33m STEP 10:   0:00:00.003501 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.411979\u001b[33m STEP 11:   0:00:01.093380 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.412327\u001b[33m STEP 12:   0:00:00.198044 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.412915\u001b[33m STEP 13:   0:00:00.056898 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.413326\u001b[33m STEP 14:   0:00:00.240361 seconds - parquet cached query\u001b[37m-----------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.413616\u001b[33m STEP 15:   0:00:00.002502 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.414094\u001b[33m STEP 16:   0:00:00.020593 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.414459\u001b[33m STEP 17:   0:00:00.006206 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.414887\u001b[33m STEP TT:   0:00:09.446510 seconds\u001b[37m----------------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.415290\u001b[33m STEP 06 Cached Query Reduction  : 46.72%\u001b[37m---------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.415767\u001b[33m STEP 14 Parquet Query Reduction : 6.59%\u001b[37m----------------------------------------------\n",
      "\u001b[37m2023-09-06 10:41:47.416381\u001b[33m STEP END OF PROGRAM-\u001b[37m-----------------------------------------------------------------\n"
     ]
    }
   ],
>>>>>>> d0fa370 (Testing)
   "source": [
    "logStep(\"TOTAL RUN TIME\")\n",
    "logStep(F\" 0:   {step00_elapsed_time} seconds\")\n",
    "logStep(F\" 1:   {step01_elapsed_time} seconds\")\n",
    "logStep(F\" 2:   {step02_elapsed_time} seconds\")\n",
    "logStep(F\" 3:   {step03_elapsed_time} seconds\")\n",
    "logStep(F\" 4:   {step04_elapsed_time} seconds\")\n",
    "logStep(F\" 5:   {step05_elapsed_time} seconds\")\n",
    "logStep(F\" 6:   {step06_elapsed_time} seconds - non-cached query\")\n",
    "logStep(F\" 7:   {step07_elapsed_time} seconds\")\n",
    "logStep(F\" 8:   {step08_elapsed_time} seconds\")\n",
    "logStep(F\" 9:   {step09_elapsed_time} seconds - cached query\")\n",
    "logStep(F\"10:   {step10_elapsed_time} seconds\")\n",
    "logStep(F\"11:   {step11_elapsed_time} seconds\")\n",
    "logStep(F\"12:   {step12_elapsed_time} seconds\")\n",
    "logStep(F\"13:   {step13_elapsed_time} seconds\")\n",
    "logStep(F\"14:   {step14_elapsed_time} seconds - parquet cached query\")\n",
    "logStep(F\"15:   {step15_elapsed_time} seconds\")\n",
    "logStep(F\"16:   {step16_elapsed_time} seconds\")\n",
    "logStep(F\"17:   {step17_elapsed_time} seconds\")\n",
    "logStep(F\"TT:   {step00_elapsed_time + step01_elapsed_time + step02_elapsed_time + step03_elapsed_time + step04_elapsed_time + step05_elapsed_time + step06_elapsed_time + step07_elapsed_time + step08_elapsed_time + step09_elapsed_time + step10_elapsed_time + step11_elapsed_time + step12_elapsed_time + step13_elapsed_time + step14_elapsed_time + step15_elapsed_time + step16_elapsed_time + step17_elapsed_time} seconds\")\n",
    "logStep(F'06 Cached Query Reduction  : {100-(step09_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(F'14 Parquet Query Reduction : {100-(step14_elapsed_time/step06_elapsed_time*100):.2f}%')\n",
    "logStep(\"END OF PROGRAM-\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
